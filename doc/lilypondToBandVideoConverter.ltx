% -*- coding: utf-8 -*- 
\documentclass[titlepage,twoside,12pt,a4paper]{book}
  \usepackage[T1]{fontenc}

  \usepackage{chngcntr}        % ==> counterwithout
  \usepackage[usenames]{color} % ==> color
  \usepackage{colortbl}        % ==> cellcolor
  \usepackage{enumitem}        % ==> ttdescription
  \usepackage{etoolbox}        % ==> patchcmd
  \usepackage{fancyhdr}        % ==> fancyhead
  \usepackage{graphicx}        % ==> includegraphics
  \usepackage{listings}        % ==> listings
  \usepackage{longtable}       % ==> longtable
  \usepackage{titlesec}        % ==> titleformat

  \usepackage{microtype}  % for better justification, requires later
                          % engine of LaTeX like e.g. pdflatex
  \usepackage{lmodern}    % scalable font

%====================
% LOCAL DEFINITIONS
%====================

  %\overfullrule=10pt

  %-- TYPEWRITER HYPHENATION --
  \DeclareFontFamily{\encodingdefault}{\ttdefault}{\hyphenchar\font=`\-}

  \renewcommand*\ttdefault{pcr}

  %------------
  %-- TITLES --
  %------------

  % chapter
  \titleformat{\chapter}
              {\normalfont \LARGE \bfseries}
              {\thechapter.~}
              {0pt}
              {\LARGE}

  % this alters "before" spacing (the second length argument) to 0
  \titlespacing*{\chapter}{0pt}{-20pt}{10pt}
  \patchcmd{\chapter}{\thispagestyle{plain}}{\thispagestyle{fancy}}{}{}

  % paragraph
  \titleformat{\paragraph}[hang]
              {\normalfont\normalsize\bfseries}
              {\theparagraph}
              {1em}
              {}
  \titlespacing*{\paragraph}{0pt}{1.5ex plus .5ex minus .2ex}{1em}

  % subparagraph
  \titleformat{\subparagraph}[hang]
              {\normalfont\normalsize\slshape}
              {\thesubparagraph}
              {1em}
              {}
  \titlespacing*{\subparagraph}{0pt}{0.8ex plus .2ex minus .1ex}{0.25em}

  \setcounter{secnumdepth}{3}
  \setcounter{tocdepth}{3}

  % number figures continously
  \counterwithout{figure}{chapter}

  %-----------------------
  %-- HEADERS / FOOTERS --
  %-----------------------

  \pagestyle{fancy}
  %--
  \fancyhead[LE]{\slshape \rightmark}
  \fancyhead[CE]{~}
  \fancyhead[RE]{~}
  \fancyhead[LO]{~}
  \fancyhead[CO]{~}
  \fancyhead[RO]{\slshape \leftmark}
  %--
  \fancyfoot[LE]{\thepage}
  \fancyfoot[CE]{~}
  \fancyfoot[RE]{Dr.~Thomas Tensi}
  \fancyfoot[LO]{LilypondToBVC, v\ltbvcVersion}
  \fancyfoot[CO]{~}
  \fancyfoot[RO]{\thepage}

  %-- page size --
  \renewcommand{\headrulewidth}{0.4pt}
  \renewcommand{\footrulewidth}{0.4pt}
  %\addtolength{\headheight}{\baselineskip}
  \addtolength{\voffset}{-5mm}
  \setlength{\topmargin}{0mm}
  \setlength{\headheight}{15pt}
  \addtolength{\headsep}{-5mm}
  \addtolength{\textheight}{20mm}

  %------------------------
  %-- LENGTH DEFINITIONS --
  %------------------------

  \setlength{\parindent}{0pt}
  \setlength{\parskip}{5pt}

  \newlength{\listingMargin}
  \setlength{\listingMargin}{4mm}
  \newlength{\listingWidth}
  \setlength{\listingWidth}{\linewidth}
  \addtolength{\listingWidth}{-2\listingMargin}

  \newlength{\listingInnerFrame}
  \setlength{\listingInnerFrame}{1mm}

  %-------------------
  %-- MISC COMMANDS --
  %-------------------

  \newcommand{\assignedTo}{\(\rightarrow\)}
  \newcommand{\bsl}{\textbackslash}

  \newcommand{\centeredExternalPicture}[2]{%
      \begin{center}
          \externalPicture{#1}{#2}%
      \end{center}
  }

  \newenvironment{centeredFigure}%
	         {\begin{figure}[tb]\begin{center}}%
	         {\end{center}\end{figure}}

  \newcommand{\embeddedCode}[1]{\textsf{#1}}
  \newcommand{\externalPicture}[2]{%
      \includegraphics[scale=#1]{figures/#2}%
  }

  \newenvironment{ttGlossary}%
	         {\begin{description}[%
                       style=nextline, labelwidth=0pt,
                       itemindent=\dimexpr-5mm, leftmargin=1cm%
                 ]}
	         {\end{description}}

  \newcommand{\glossaryLink}[1]{\(\rightarrow\)\textit{#1}}
  \newcommand{\hyperlink}[1]{\textsf{\small \color{blue}#1}}
  \newcommand{\ltbvc}{Lilypond\-To\-Band\-Video\-Converter}
  \newcommand{\ltbvcCommand}{lilypondToBVC}
  \newcommand{\ltbvcVersion}{1.1.1}
  \newcommand{\macro}[1]{\textbackslash #1}
  \newcommand{\mandatory}{\textbf{MANDATORY}}
  \newcommand{\meta}[1]{\guillemotleft #1\guillemotright}
  \newcommand{\oneover}[1]{{\tiny 1/}\embeddedCode{#1}}
  \newcommand{\placeholder}[1]{\embeddedCode{\$\{#1\}}}
  \newcommand{\TODO}[1]{\emph{\color{red}\textbf{TODO:} #1}}

  \newenvironment{ttDescription}%
	         { \begin{description}[%
                          style=nextline, labelwidth=0pt,
                          itemindent=\dimexpr-5mm, leftmargin=1cm ] }
	         { \end{description} }

  \definecolor{variableTableHeadingBkgnd}{RGB}{200,200,200}
  \definecolor{variableTableStandardBkgnd}{RGB}{255,255,240}

  \newenvironment{variableDescriptionTableFinal}
                 {\begin{center}\scriptsize
                    \begin{longtable}{|p{3.3cm}|p{5.6cm}|p{2.5cm}|p{0.6cm}|}
                    \hline
                 }
                 {  \end{longtable}
                  \end{center}}

  \newenvironment{variableDescriptionTableLong}
                 {\begin{center}\scriptsize
                    \begin{tabular}{|p{3.3cm}|p{6.0cm}|p{2.8cm}|}\hline
                 }
                 {  \end{tabular}
                  \end{center}}

  \newenvironment{variableDescriptionTableShort}
                 {\begin{center}\scriptsize
                      \begin{tabular}{|p{3.3cm}|p{9.1cm}|}\hline
                         \vdtHeadingLineShort{\textbf{Variable}}
                                             {\textbf{Description}}
                 }
                 {   \end{tabular}
                  \end{center}}

  \newcommand{\varDescriptionFinal}[4]{%
      \cellcolor{variableTableStandardBkgnd}\embeddedCode{#1}
      &\cellcolor{variableTableStandardBkgnd}#2
      &\cellcolor{variableTableStandardBkgnd}#3
      &\cellcolor{variableTableStandardBkgnd}
        \mbox{\hfil\ref{figure:configFile#4Variables}\hfil}\\\hline
  }

  \newcommand{\varDescriptionHeaderFinal}{
      \vdtHeadingLineFinal{\textbf{Variable}}
                          {\textbf{Description}}
                          {\textbf{Default}}
                          {\textbf{Fig.}}
      \endhead
  }

  \newcommand{\varDescriptionHeaderLong}{
      \vdtHeadingLineLong{\textbf{Variable}}
                         {\textbf{Description}}
                         {\textbf{Example}}
  }

  \newcommand{\varDescriptionHeaderShort}{
      \vdtHeadingLineShort{\textbf{Variable}}
                          {\textbf{Description}}
  }

  \newcommand{\varDescriptionLong}[3]{%
      \cellcolor{variableTableStandardBkgnd}\embeddedCode{#1}
      &\cellcolor{variableTableStandardBkgnd}#2
      &\cellcolor{variableTableStandardBkgnd}#3\\\hline
  }

  \newcommand{\varDescriptionShort}[2]{%
      \cellcolor{variableTableStandardBkgnd}\embeddedCode{#1}
      &\cellcolor{variableTableStandardBkgnd}#2\\\hline
  }

  \newcommand{\vdtHeadingLineFinal}[4]{%
      \cellcolor{variableTableHeadingBkgnd}#1
      &\cellcolor{variableTableHeadingBkgnd}#2
      &\cellcolor{variableTableHeadingBkgnd}#3
      &\cellcolor{variableTableHeadingBkgnd}#4\\\hline
  }

  \newcommand{\vdtHeadingLineShort}[2]{%
      \cellcolor{variableTableHeadingBkgnd}#1
      &\cellcolor{variableTableHeadingBkgnd}#2\\\hline
  }

  \newcommand{\vdtHeadingLineLong}[3]{%
      \cellcolor{variableTableHeadingBkgnd}#1
      &\cellcolor{variableTableHeadingBkgnd}#2
      &\cellcolor{variableTableHeadingBkgnd}#3\\\hline
  }

  %-------------------------
  %-- LISTING DEFINITIONS --
  %-------------------------

  \definecolor{configFileBkgnd}   {RGB}{230,250,230}
  \definecolor{lilypondFileBkgnd} {RGB}{230,230,250}
  \definecolor{fileTextBkgnd}     {RGB}{240,240,240}

  \newcommand{\listTitleBox}[1]{%
    \colorbox{fileTitleTextBkgnd}%
             {\parbox{\listingWidth}%
                     {\color{fileTitleTextColor}\bf #1}%
             }%
  }

  \lstdefinestyle{standard}
                 {gobble=2, showspaces=false, showstringspaces=false,
                  xleftmargin=\listingMargin,
                  xrightmargin=\listingMargin,
                  framexleftmargin=\listingInnerFrame,
                  framextopmargin=\listingInnerFrame,
                  framexrightmargin=\listingInnerFrame,
                  framexbottommargin=\listingInnerFrame,
                  basicstyle=\scriptsize\ttfamily}

  \lstdefinelanguage{ltbvc}{
      morekeywords = {INCLUDE},
      sensitive=true,
      morecomment=[l]{--},
      morestring=[b]"
  }

  \def\ttlisting{\par\minipage{\linewidth}\lstset}
  \def\endttlisting{\endminipage\par}
  %\def\ttlisting{\par\lstset}
  %\def\endttlisting{\par}

  \lstnewenvironment{commandLine}
                    {\ttlisting{style=standard}}
                    {\endttlisting}

  \lstnewenvironment{configurationFileCode}
                    {\ttlisting{language=ltbvc,style=standard,
                                backgroundcolor=\color{configFileBkgnd}}}
                    {\endttlisting}

  \lstnewenvironment{verbatimText}
                    {\ttlisting{language=ltbvc, style=standard,
                                backgroundcolor=\color{fileTextBkgnd}}}
                    {\endttlisting}

  \lstnewenvironment{lilypondFileCode}
                    {\ttlisting{language=ltbvc, style=standard,
                                backgroundcolor=\color{lilypondFileBkgnd}}}
                    {\endttlisting}

%################
%# REUSED TEXTS #
%################

\newcommand{\DESCaacCommandLine}{%
  aac encoder command line with parameters for input
  (\placeholder{infile}) and output (\placeholder{outfile}) (optional,
  if not defined ffmpeg is used for aac encoding)
}

\newcommand{\DESCalbumName}{%
  album for song group (embedded as ``album'' in audio and video
  files)
}

\newcommand{\DESCartistName}{%
  artist of that song group (embedded as ``artist'' and ``album
  artist'' in audio and video files)
}

\newcommand{\DESCaudioGroupToVoicesMap}{%
  mapping from freely defined voice group names to names of voices
  contained in that group described by a slash-separated name list
}

\newcommand{\DESCaudioProcessorChainSeparator}{%
  string or character used for separating audio chains within audio
  refinement effects; defaults to ";"
}

\newcommand{\DESCaudioProcessorMixingCommandLine}{%
  audio processor command line for mixing audio files with volume
  factors containing \placeholder{factor}, \placeholder{pan},
  \placeholder{infile} and \placeholder{outfile} as placeholders; the
  group of factor and infile is embraced by parentheses ("[]") and
  will be repeated depending on the number of infiles with the
  parentheses removed; if missing, mixing will be done by (slow)
  internal routines, if ``pan'' is not specified as a placeholder, an
  internal panning via ffmpeg is done
}

\newcommand{\DESCaudioProcessorAmplificationEffect}{%
  audio processor command for amplifying audio by some dB value
  containing \placeholder{amplificationLevel} as placeholder
}

\newcommand{\DESCaudioProcessorPaddingCommandLine}{%
 audio processor command line for padding an audio files with leading
 silence containing \placeholder{duration} (in seconds),
 \placeholder{infile} and \placeholder{outfile} as
 placeholders; if missing, padding will be done by (slow) internal
 routines
}

\newcommand{\DESCaudioProcessorRedirector}{%
  string or character used for specifying special inputs or outputs
  within audio refinement effects; defaults to "->"
}

\newcommand{\DESCaudioProcessorRefinementCommandLine}{%
  audio processor command line for audio refinement with parameters
  for input (\placeholder{infile}), output (\placeholder{outfile}) and
  the refinement effects (\placeholder{effects})
}

\newcommand{\DESCaudioTargetDirectoryPath}{%
  path for the final AAC audio files with subsets of rendered and
  refined audio tracks
}

\newcommand{\DESCaudioTrackAlbumName}{%
  name of the album of the audio file for given list of voices (where
  an embedded dollar-sign is replaced by the global album name)
}

\newcommand{\DESCaudioTrackAmplificationLevel}{%
  decimal value in decibels telling the volume change to be applied to
  a track audio file; this is helpful to adjust volume levels of
  different songs within an album
}

\newcommand{\DESCaudioTrackAudioFileTemplate}{%
  template string defining how the audio file name of the target audio
  file for given list of voices is constructed from the plain audio
  file name (indicated by a dollar-sign)
}

\newcommand{\DESCaudioTrackDescription}{%
  description for audio track within target video (typically
  unsupported by video players)
}

\newcommand{\DESCaudioTrackGroupList}{%
  slash-separated list of audio group names occuring as keys in
  \embeddedCode{audioGroupToVoicesMap}
}

\newcommand{\DESCaudioTrackLanguageCode}{%
  ISO language code for audio track within target video (typically
  supported by video players)
}

\newcommand{\DESCaudioTrackMasteringEffectList}{%
  list of audio track specific refinement effects to be applied after
  voice mixdown
}

\newcommand{\DESCaudioTrackSongNameTemplate}{%
  template string defining how the song name for given list of voices
  is constructed from the plain song name (indicated by a
  dollar-sign)
}

\newcommand{\DESCaudioTrackVoiceNameToMixSettingMap}{%
  mapping from voice names to volume factors and pan positions used
  for mixing the refined audio files into cumulated audio file for
  given track with both elements separated by a slash; the factors
  are decimal values in decibels (where 0.0 means that the refined
  voice file is taken without change with a conversion of \(10^{{\rm
  dBValue}/20}\)), the pan position is given as a decimal value
  between 0 and 1 with suffix ``R'' or ``L'' (for right/left) or the
  character ``C'' (for center)
}

\newcommand{\DESCaudioTrackList}{%
  list of track descriptors defining groups of audio groups to be put
  on some track with naming templates for audio file, song and album
  name and a track description and language
}

\newcommand{\DESCaudioVoiceNameSet}{%
  set of voice names to be rendered to audio files via the phases
  ``rawaudio'' and ``refinedaudio'' based on voice representations in
  humanized midi file
}

\newcommand{\DESCcomposerText}{%
  composer text to be shown in voice extracts and score
}

\newcommand{\DESCcountInMeasureCount}{%
  number of count-in measures for the song (which defines the time
  before the first measure)
}

\newcommand{\DESCincludeFilePath}{%
  path for the music include file containing all fragments for
  lilypond processing; if unset, defaults to
  \embeddedCode{fileNamePrefix} plus ``-music.ly''
}

\newcommand{\DESCintermediateFilesAreKept}{%
  boolean telling whether temporary files are kept
}

\newcommand{\DESCintermediateFileDirectoryPath}{%
  path of directory where intermediate files go that are either used
  for processing within a phase or as information between phases
}

\newcommand{\DESCextractVoiceNameSet}{%
  set of voices to be rendered as a voice extract
}

\newcommand{\DESCffmpegCommand}{%
  location of ffmpeg command
}

\newcommand{\DESCfileNamePrefix}{%
  file name prefix used for all generated files for this song
}

\newcommand{\DESChumanizationStyleXXX}{%
  map that tells the initial count-in measures, the variation in
  timing and velocity for several positions within a measure
}

\newcommand{\DESChumanizedVoiceNameSet}{%
  set of voice names to be humanized by random variations of timing
  and velocity
}

\newcommand{\DESClilypondCommand}{%
  location of lilypond command
}

\newcommand{\DESClilypondVersion}{%
  the version string for lilypond
}

\newcommand{\DESCloggingFilePath}{%
  path of file containing the processing log (potentially overridden
  by the \embeddedCode{-l} option on the command-line
}

\newcommand{\DESCmeasureToHumanizationStyleNameMap}{%
  map of measure number to humanization style name used from this
  position onward for humanized voices; if map is empty, no
  humanization is done
}

\newcommand{\DESCmeasureToTempoMap}{%
  map defining the tempo for measure in bpm until another tempo
  setting is given; the time signature as a fraction may be appended
  after a vertical bar (4/4 is default)
}

\newcommand{\DESCmidiChannelList}{%
  list of midi channels per voice each between 1 and 16 (10 for a drum
  voice)
}

\newcommand{\DESCmidiInstrumentList}{%
  list of midi instrument programs per voice each as an integer
  between 0 and 127; each entry may be prefixed by a bank number (0 to
  127) followed by a colon
}

\newcommand{\DESCmidiToWavRenderingCommandLine}{%
  command line for rendering command from MIDI file to WAV audio file
  (typically ``fluidsynth'' with parameters for input
  (\placeholder{infile}) and output (\placeholder{outfile}))
}

\newcommand{\DESCmidiVoiceNameList}{%
  list of voices to be rendered in order given into the MIDI file
}

\newcommand{\DESCmidiVolumeList}{%
  list of midi volumes per voice each as an integer between 0 and 127
}

\newcommand{\DESCmpfourboxCommand}{%
  location of mp4box command (if available); if empty ffmpeg is used
  instead
}

\newcommand{\DESCpanPositionList}{%
  list of pan positions per voice as a decimal value between 0 and 1
  with suffix ``R'' or ``L'' (for right/left) or the character ``C''
  (for center)
}

\newcommand{\DESCparallelTrack}{%
  specification of an audio file name, a volume factor (in decibels)
  and an offset (in seconds) relative to the start of the song for an
  audio track to be added to all audio submixes (e.g. for pre-rendered
  audio)
}

\newcommand{\DESCphaseAndVoiceNameToClefMap}{%
  mapping from processing phase to maps from voice name to lilypond
  clef
}

\newcommand{\DESCphaseAndVoiceNameToStaffListMap}{%
  mapping from processing phase to maps from voice name to
  slash-separated lilypond staff names
}

\newcommand{\DESCreverbLevelList}{%
  list of reverb levels (as decimal values typically between 0 and 1)
  for the voices aligned with the list \embeddedCode{voiceNameList};
  those reverb levels are applied to each voice as the final
  refinement operation (when the sox audio processor is used)
}

\newcommand{\DESCscoreVoiceNameList}{%
  list of voices to be rendered in order given into the score
}

\newcommand{\DESCsoundStyleXXX}{%
  sequence of refinement effects (typically from sox) to be applied
  on raw audio file when this style is selected for \meta{voice}
}

\newcommand{\DESCsoundVariantList}{%
  list of variant names for the sound styles of the voices aligned
  with the list \embeddedCode{voiceName}; those style variant names
  are combined into a complete style name to be applied during audio
  refinement
}

\newcommand{\DESCtargetDirectoryPath}{%
  path of directory where all generated files go (except for audio and
  video files)
}

\newcommand{\DESCtempAudioDirectoryPath}{%
  path of directory for temporary audio files
}

\newcommand{\DESCtempLilypondFilePath}{%
  path of temporary lilypond file containing placeholders for
  \placeholder{phase} and \placeholder{voiceName}
}

\newcommand{\DESCtitle}{%
  human visible title of song used as tag in the target audio file and
  as header line in the notation files
}

\newcommand{\DESCtrackNumber}{%
  track number within album
}

\newcommand{\DESCvoiceNameToChordsMap}{%
  mapping from voice names to phase abbreviations where chords are
  shown for that voice system
}

\newcommand{\DESCvoiceNameToLyricsMap}{%
  mapping from voice name to a count of parallel lyrics lines directly
  following the target letter (``e'' for the extract, ``s'' for the
  score and ``v'' for the video)
}

\newcommand{\DESCvoiceNameToScoreNameMap}{%
  mapping from voices name to short score name at the beginning of a
  system
}

\newcommand{\DESCyear}{%
  year of arrangement
}

\newcommand{\DESCvideoFileKindDirectoryPath}{%
  directory where final videos for that target go
}

\newcommand{\DESCvideoFileKindFileNameSuffix}{%
  suffix to be used for the video file names for that target
}

\newcommand{\DESCvideoFileKindTarget}{%
  name of associated video target that is used when rendering video
  files of that kind
}

\newcommand{\DESCvideoFileKindVoiceNameList}{%
  list of voice names to be rendered in order to audio files via the
  phase ``silentvideo''
}

\newcommand{\DESCvideoFileKindMap}{%
  mapping from video file kind name to video file kind descriptor with
  several parameters for specific video file generation referencing
  a video target that gives overall video parameters
}

\newcommand{\DESCvideoTargetFFMpegPresetName}{%
  a specific ffmpeg preset for the current video target device (a string,
  a missing value defaults to a baseline level 3 profile)
}

\newcommand{\DESCvideoTargetFrameRate}{%
  the frame rate of the video (in frames per second)
}

\newcommand{\DESCvideoTargetMediaType}{%
  the Quicktime media type of the video (for example "TV Show")
}

\newcommand{\DESCvideoTargetHeight}{%
  height of device and video (in dots)
}

\newcommand{\DESCvideoTargetLeftRightMargin}{%
  margin for video on left and right side (in millimeters)
}

\newcommand{\DESCvideoTargetResolution}{%
  resolution of the device (in dpi)
}

\newcommand{\DESCvideoTargetScalingFactor}{%
  the factor (an integer) by which width and height are multiplied for
  lilypond image rendering to be later downscaled for a better edge
  smoothing via antialiasing
}

\newcommand{\DESCvideoTargetSubtitleColor}{%
  color of overlayed subtitle in final video for measure display (as
  integer for 16bit alpha/red/green/blue)
}

\newcommand{\DESCvideoTargetSubtitlesAreHardcoded}{%
  flag to tell whether subtitles are burnt into the video or are
  available as a separate subtitle track
}

\newcommand{\DESCvideoTargetSubtitleFontSize}{%
  height of subtitle (in pixels)
}

\newcommand{\DESCvideoTargetSystemSize}{%
  size of lilypond system (in lilypond units, cf. lilypond system
  size)
}

\newcommand{\DESCvideoTargetTopBottomMargin}{%
  margin for video on top and bottom (in millimeters)
}

\newcommand{\DESCvideoTargetWidth}{%
  width of device and video (in dots)
}

\newcommand{\DESCvideoTargetMap}{%
  mapping from video target name to video target descriptor with
  several parameters for specific video file generation
}

\newcommand{\DESCvoiceNameToOverrideFileNameMap}{%
  map from voice name to name of file overriding that voice in the
  processed audio files and in the final mixdown audio files and in
  the target videos
}

\newcommand{\DESCvoiceNameToVariationFactorMap}{%
  map from voice name to a pair of decimal factors characterizing the
  timing and velocity variation for this kind of voice to be applied
  additional to the humanization style
}
%############################################################
\begin{document}

\title{\textbf{\Huge \ltbvc}\\[3mm]
       Automated Generation of Notation Videos with Backing
       Tracks (v\ltbvcVersion)}
\author{Dr.~Thomas Tensi}
\date{\today}
\maketitle

\tableofcontents

\newpage
\listoffigures

% allow a more relaxed text justification
\tolerance=500

%=====================
\chapter{Introduction}
%=====================

%-----------------
\section{Overview}
%-----------------

The \emph{\ltbvc} is an application consisting of several python
scripts that orchestrate standard command-line tools to convert a
music piece (a song) written in the lilypond notation to

\begin{itemize}
  \item a PDF score of the whole song,
  \item several PDF voice extracts,
  \item a MIDI file with all voices (with additional preprocessing
        applied to achieve some humanization),
  \item audio mix files with several subsets of voices (specified
        by configuration), and
  \item video files for several output targets visualizing the score
        notation pages and having the mixes as mutually selectable
        audio tracks as backing tracks.
\end{itemize}

The central aim is to finally have a video file with several audio
tracks containing mixes of different voice subsets to be used as
selectable backing tracks.  The video itself shows a score with
``pages'' turned at the right time and an indication of the current
measure as a subtitle.

So one might have a score video to be displayed on some device (like a
tablet) that synchronously plays, for example, a backing track without
vocals, guitar and keyboard, but with bass and drums.  Hence a
(partial) band can play the missing voices live (reading the score)
and have the other voices coming from the backing track.

For processing a song one must have
\begin{itemize}

  \item a lilypond include file with the score information containing
        specific lilypond identifiers, and

  \item a configuration file giving details like the voices occuring
        in the song, their associated midi instrument, target audio
        volume, list of mutable voices for the audio tracks
        etc.

\end{itemize}

Based on those files the python script --~together with some
open-source command-line software like ffmpeg~-- produces all the
target files. This is done either incrementally or altogether
depending on command-line settings for the script.

In principle, all this could also be done with standard lilypond files
using command line tools.  But the \ltbvc\ application automates a lot
of that: based on data given in a song-dependent configuration file
plus the lilypond fragment file for the notes of the voices, it adds
boilerplate lilypond code, parametrizes the tool chain and calls the
necessary programs automatically.  And the process is completely
unattended: once the required configuration and lilypond notation
files are set up the process runs on its own.  Additionally the audio
generation can be tweaked by defining midi humanization styles and
command chains (``sound styles'') for the audio postprocessing.

This document assumes that you have an adequate knowledge of the
following underlying software:
\begin{ttDescription}
  \item [lilypond:] for the notation specification,
  \item [sox:] for postprocessing the audio files
\end{ttDescription}

%---------------------------------
\section{Outline of this Document}
%---------------------------------

This document will present how to setup a lilypond fragment file and
an associated configuration file for processing with \ltbvc.

\begin{itemize}

  \item Chapter~\ref{section:preliminaries} describes the installation
        requirements and defines some terminology used in this
        document.

  \item Chapter~\ref{section:programUsage} tells how the (command line)
        program is used and what kind of processing phases are available.
        There is also some dependency between the artifacts of the phases
        that is presented there.

  \item Chapter~\ref{section:configurationFileOverview} gives an overview
        of the syntax of a \ltbvc\ configuration file.  It consists of
        key-value-pairs; the keys are identifiers, but the values may
        be a bit more complicated.

  \item Chapter~\ref{section:lilypondFileOverview} tells how the
        lilypond fragment file should look.  Of course, the syntax is
        given by the lilypond program, but ---~since we have fragments
        with external boilerplate code~--- we discuss what kind of
        information must be provided in those files.

  \item Chapter~\ref{section:configurationFileSettings} discusses in
        detail each configuration file variable needed by going through
        all the processing phases in sequence.

  \item Chapter~\ref{section:example} gives an example by showing all
        the lilypond macros and all required configuration settings
        for a simple two-verse blues song with three instruments.  It
        shows that some initial effort is needed, but normally you can
        reuse things once you have understood how to make it work.

  \item Because things will certainly go wrong some time,
        chapter~\ref{section:debugging} gives some hints on how to
        trace the problem.

  \item Further bibliographic information and links to the tools are
        given in chapter~\ref{section:references}.
        
  \item Appendix~\ref{section:configurationFileVariableList} gives
        an overview table of all configuration file commands,
        appendix~\ref{section:glossary} gives a glossary of terms,
        appendix~\ref{section:releaseChanges} shows all the release
        changes.

\end{itemize}

%============================
\chapter{Preliminaries}
\label{section:preliminaries}
%============================

%---------------------
\section{Requirements}
%---------------------

All the scripts are written in python and can be installed as a python
package.  The package requires either Python 2.7 or Python 3.3 or
later and relies on the python package mutagen.

Additionally the following software must be available:
\begin{ttDescription}

  \item [lilypond:] for generating the score pdf, voice extract pdfs,
        the raw midi file and the score images used in the video files
        \cite{reference:lilypondDocumentation},

  \item [ffmpeg:] for video generation and video postprocessing
        \cite{reference:ffmpegDocumentation},

  \item [fluidsynth:] for generation of voice audio files from a midi
        file \cite{reference:fluidsynthDocumentation} plus some
        soundfont (e.g. FluidR3\_GM.sf3 at
        \cite{reference:fluidSoundfontOriginal} or
        \cite{reference:fluidSoundfontMuseScore}), and

  \item [sox:] for instrument-specific postprocessing of audio files
        for the target mix files as well as the mixdown
        \cite{reference:soxDocumentation}
\end{ttDescription}

Both ``fluidsynth'' and ``sox'' may be replaced by other software that
does similar file transformations.  ``fluidsynth'' can be substituted
by a command-line program transforming MIDI to WAV, ``sox'' by one
doing command-line audio processing on WAV files.  In both cases the
corresponding configuration has to be adapted accordingly.

The following software is optional:
\begin{ttDescription}
  \item [aac:] an AAC-encoder for the final audio mix
        file compression (for example
        \cite{reference:aacDocumentation}), and

  \item [mp4box:] the MP4 container packaging software mp4box
        \cite{reference:mp4boxDocumentation}
\end{ttDescription}

The location of all those commands as well as a few other settings has
to be defined in a global configuration file for the \ltbvc\
(cf. overall configuration file syntax)

%---------------------------
\section{Installation}
\label{section:installation}
%---------------------------

The program is available via the Python platform PyPi, the Python
package index.

\begin{commandLine}
  pip install lilypondToBandVideoConverter
\end{commandLine}

Once installed the program is ready for use.  Make sure that the
scripts directory of python is in the path for executables on your
platform.

%===========================
\chapter{Terminology}
%===========================

Because the different programs do not completely agree in their
terminology, a single terminology is used throughout the document and
this is defined here.  Appendix~\ref{section:glossary} gives a
detailed description of the all terms used in this document.

The most important terms are:
\begin{ttDescription}

  \item [voice:]
        a polyphonic part of a composition belonging to a single
        instrument to be notated in one or several musical staffs

  \item [song:]
        a collection of several parallel voices forming a musical piece

  \item [album:]
        a collection of several related songs (for example, related by
        year, artist, etc.)

  \item [audio track:]
        the audio rendering of a subset of all song voices (typically
        within the final notation video)

\end{ttDescription}

%===========================
\chapter{Usage}
\label{section:programUsage}
%===========================

The \ltbvc\ is a commandline program with the following syntax:
\begin{commandLine}
  lilypondToBVC [-h] [-k] [-l loggingFilePath] --phases PHASELIST
                [--voices VOICELIST] configurationFilePath
\end{commandLine}

The options have the following meaning:
\begin{ttDescription}

  \item [-h]
        makes the program show all the commandline options and exit

  \item [-k]
        force the program to keep intermediate files

  \item [-l loggingFilePath]
        gives the path for the logging file (overriding any corresponding
        setting in the configuration file)

  \item [-\relax-phases PHASELIST]
        specifies the processing phases or combination of processing
        phases to be applied; is a slash-separated identifier list
        from the set\linebreak
        \embeddedCode{ \{all, preprocess, postprocess, extract, score,
          midi, silentvideo, rawaudio, refinedaudio, mix,
          finalvideo\}} defined here

  \item [-\relax-voices VOICELIST]
        gives the slash-separated list of voices where current phase
        should be done on (for example, only on vocals and on drums);
        those voice names should be a subset of the list of voices
        given in the configuration file and in the associated lilypond
        fragment file; this option is optional: when it is not given,
        all voices are used; only applies to phases ``extract'',
        ``rawaudio'' and ``refinedaudio''

  \item [configurationFilePath]
        gives the path to the configuration file specifying all
        information about the song to be processed

\end{ttDescription}

The several processing phases of \ltbvc\ produce the several outputs
incrementally and are named by the kind of result they produce.  Those
phases have the following meanings:

\begin{ttDescription}

  \item [extract:]
        generates PDF notation files for single voices as extracts
        (might use compacted versions if specified),

  \item [score:]
        generates a single PDF file containing all voices as a score,

  \item [midi:]
        generates a MIDI file containing all voices with specified
        instruments, pan positions and volumes,

  \item [silentvideo:]
        generates (intermediate) silent videos containing the score
        pages for several output video file kinds (with configurable
        resolution and size),

  \item [rawaudio:]
        generates unprocessed (intermediate) audio files for all the
        instrument voices from the midi tracks,

  \item [refinedaudio:]
        generates (intermediate) audio files for all the instrument
        voices with additional audio processing applied,

  \item [mix:]
        generates final compressed audio files with submixes of all
        instrument voices based on the refined audio files with
        a specified volume balance and some subsequent mastering audio
        processing (where the submix variants are configurable), and

  \item [finalvideo:]
        generates a final video file with all submixes as selectable
        audio tracks and with a measure indication as subtitle

\end{ttDescription}

\begin{centeredFigure}
  \centeredExternalPicture{0.65}{dependencyDiagram.mps}
  \caption{Dependencies between Generation Phases}
  \label{figure:phaseDependencies}
\end{centeredFigure}

Of course, those phases are not independent.  Several phases rely on
results produced by other phases.
Figure~\ref{figure:phaseDependencies} shows how the phases depend on
each other.  The files (in yellow) are generated by the phases (in
magenta), the configuration file (in green) and the lilypond fragment
file (in blue) are the only manual inputs into the processing chain.

For example, the phase \embeddedCode{rawaudio} needs a midi file as
input containing all voices to be rendered as audio files.  When using
combining phases (see below) or when specifying several phases for a
single run of the \ltbvc\ application, the phases are processed in a
correct order, but when doing a manual selection of phases, you have
to make sure that the dependencies given are obeyed.

In the following we shall use the color coding for the files as given
in figure~\ref{figure:phaseDependencies}: parts from the configuration
file have a green background, parts from the lilypond fragment file
have a blue background.

There are also some combining phase available as follows:

\begin{ttDescription}

  \item [preprocess:]
        combining all the phases \embeddedCode{extract},
        \embeddedCode{score}, \embeddedCode{midi} and
        \embeddedCode{silentvideo} for generation of voice extract
        PDFs and score PDF, MIDI file as well the silent videos for
        all video file kinds

  \item [postprocess:]
        combining all the phases \embeddedCode{rawaudio},
        \embeddedCode{refinedaudio}, \embeddedCode{mix} and
        \embeddedCode{finalvideo} for generation of the intermediate
        raw and refined WAV files, the submixes as compressed audios
        and the final videos for all video file kinds

  \item [all:]
        full processing via phase groups \embeddedCode{preprocess} and
        \embeddedCode{postprocess}

\end{ttDescription}

So for example
\begin{commandLine}
  lilypondToBVC --phases voice/score
                --voices vocals/strings/drums config.txt
\end{commandLine}

will generate the voice extracts for vocals, strings and drums as well
as a song score with those three voices specified in file
\embeddedCode{config.txt}.  The vertical order within the score as
well as other layout parameters are given by the order of voice
descriptions and specific variables in the configuration file.

%========================================
\chapter{Configuration File Overview}
\label{section:configurationFileOverview}
%========================================

The song processing is controlled by several variables; those have to
be defined in the configuration file for a song.  The name of this
file is given as a mandatory parameter for the application.

Note that typically there is not a single configuration file, but
several.  Often a song configuration file includes others with global
definitions (like, for example, defining the location of the ffmpeg
command or some style of audio postprocessing).

Although there is some internal program logic separating the variables
into different domains for global setup variables, album related
variables and song variables, this is somewhat academical: a variable
definition can be given at any place and a later definition overrides
a previous one.

%------------------------------------
\section{Configuration File Location}
%------------------------------------

The configuration file(s) are searched for in the following locations
in the given order:
\begin{itemize}

  \item the current directory

  \item the directory ~/.ltbvc within the user's home directory

  \item the directory config and ../config relative to the directory
        of the python program files

\end{itemize}

%--------------------------------------
\section{Configuration File Syntax}
\label{section:configurationFileSyntax}
%--------------------------------------

Each configuration file has a simple line-oriented syntax as follows:
\begin{itemize}

  \item Leading and trailing whitespace in a line is ignored.
        Other whitespace is only interpreted as token separator.

  \item A line starting with a comment marker ``{\tt -\,-}''
        (double-dash) is ignored.

  \item Each relevant line starts with an identifier followed by an
        equal sign and the associated value.  The associated value may
        be an integer, a decimal, a boolean or a string.  By this
        assignment the value is associated with the variable given by
        the identifier.  A subsequent assignment to the same
        variable will replace that value.

  \item An identifier is a sequence of lower- and uppercase letters or
        underscores and signifies a variable.  One may define such
        variables arbitrarily.

  \item Several physical lines are collected into a single logical
        value assignment line until either an empty line (with only
        whitespace) or a new assigment line is encountered.

  \item A line may end with a continuation marker
        ``\textbackslash''.  That marker is discarded
        and the line is combined into the previous logical assignment
        line (if any).

  \item An integer literal is a digit sequence, a decimal value is a
        digit sequence with at most one decimal point, a boolean value
        is either the string ``true'' or ``false'' and a string value is
        a character sequence enclosed by double quotes.  Two double
        quotes within a string are interpreted as a double quote
        character.

  \item When a variable identifier occurs on the right hand side of an
        assignment, it is \emph{immediately} replaced by its
        associated value.  If there is none, this is an error.  The
        processing is strictly sequential: the use of an identifier
        \emph{must occur after its definition}.  It is okay to use an
        identifier in its own redefinition or to have more than one
        definitions of an identifier.

  \item A sequence of adjacent string literals or variables with
        string contents are concatenated into a single string value.

  \item A line starting with ``INCLUDE'' followed by a string
        specifies the name of a file to be included in place.

  \item As a convention sets have comma-separated string values and
        maps are strings with a leading and trailing brace and key and
        values separated by a colon.  White space within those strings
        is not significant except when it is itself part of a value
        string enclosed in single quotation marks.

  \item It is helpful to distinguish auxiliary variables from those
        used by the program.  As a simple convention in this document
        we prefix auxiliary variables with an underscore (but any
        convention ---~even none~--- is fine).

\end{itemize}

Assume for an example the following definitions in two files
``test.text'' and ``config.txt'':

\begin{configurationFileCode}
  -- test.txt file to be included elsewhere
  voiceNameList = "vocals, guitar, drums"
  humanizedVoiceNameSet = "vocals"
  _initialTempo = "90"
  year = 2021
\end{configurationFileCode}

\begin{configurationFileCode}
  -- config.txt file including test file
  INCLUDE "test.txt"
  voiceNameList = "vocals, guitar"
  humanizedVoiceNameSet = humanizedVoiceNameSet ", drums"
  measureToTempoMap = "{ 1 : " _initialTempo ", 20 : 67 }"
\end{configurationFileCode}

leads to the following overall variable settings:

\begin{verbatimText}
  _initialTempo         = "90"
  year                  = 2021
  voiceNameList         = "vocals, guitar"
  humanizedVoiceNameSet = "vocals, drums"
  measureToTempoMap     = "{ 1 : 90, 20 : 67 }"
\end{verbatimText}

%========================================
\chapter{Lilypond Fragment File Overview}
\label{section:lilypondFileOverview}
%========================================

The lilypond fragment file used for a song contains lilypond macros.
We do not discuss the details of the lilypond language here, it is
recommended to have a look into the lilypond
documentation\cite{reference:lilypondDocumentation}.

At least there must be definitions for the following items in the
lilypond file:

\begin{ttDescription}

  \item [\embeddedCode{keyAndTime}:]
        tells the key and time of the song and assumes that this
        applies to all voices

  \item [\embeddedCode{\meta{voice}XXX}:]
        for each voice given in the configuration file containing the
        musical expression to be used in an extract, in a score, in
        the midi file or in the video; here ``XXX'' depends on the
        target, so you might have different macros for a voice for the
        different targets it occurs in (extract, score, midi, video).

\end{ttDescription}

The names of all voices are given by the configuration variable
\embeddedCode{voiceNameList}.  Because lilypond only allows letters in
macro names, those voice names must consist of small and capital
letters only (no blanks, no digits, no special characters!) and they
are case sensitive.  And they should not clash with predefined lilypond
macros
\footnote{Like \embeddedCode{drums}, but because this is a common
          voice name it is automatically mapped to
          \embeddedCode{myDrums} by the generator.}.

The above looks quite complicated because you need macros for each
voice and each processing phase.  But often you will reuse lilypond
macros and typically the MIDI macro \embeddedCode{\meta{voice}Midi} is
the same as the score macro \embeddedCode{\meta{voice}} only with
\emph{all repetitions unfolded}.  You do not have to do this by
yourself: for midi output this unfolding is done by the generator.

There is even another automatism: if the generator looks for some
voice macro with some extension it also accepts the plain macro for
the voice (if available).  For example, if the macro
\embeddedCode{guitarMidi} cannot be found, the generator looks for the
macro \embeddedCode{guitar} and automatically applies necessary
lilypond transformations (like unfolding repeats).

\begin{centeredFigure}
  \begin{variableDescriptionTableLong}

    \vdtHeadingLineLong{\textbf{Config. Variable}}
                       {\textbf{Description}}
                       {\textbf{Lilypond Var.}}

    \varDescriptionLong{audioVoiceNameSet}
                       {for each voice given in the set the lilypond
                        macro gives the musical expression for the
                        voice to be rendered as an audio file with
                        the voice name}
                       {\meta{voice}Midi}

    \varDescriptionLong{extractVoiceNameSet}
                       {for each voice given in the list the lilypond
                        macro gives the musical expression for a
                        voice to be rendered in the corresponding
                        voice extract}
                       {\meta{voice}Extract}

    \varDescriptionLong{midiVoiceNameList}
                       {for each voice given in the list the lilypond
                        macro gives the musical expression for the
                        voice to be rendered in the \emph{midi file}
                        and rendered as an audio file with the voice
                        name; the list is the order of the voices in
                        the file}
                       {\meta{voice}Midi}

    \varDescriptionLong{scoreVoiceNameList}
                       {for each voice given in the list the lilypond
                        macro gives the musical expression for the
                        voice to be rendered in the \emph{midi
                        file}, the list is the order of the voices
                        in the score from top to bottom}
                       {\meta{voice}Score}

  \end{variableDescriptionTableLong}

  \caption{Dependency of Lilypond Macros on Configuration Variables}
  \label{figure:lilypondDependencyOnConfig}
\end{centeredFigure}

There is a connection between the lilypond and the configuration file:
some variables in the configuration file make some lilypond macros
``mandatory''.  The table in
figure~\ref{figure:lilypondDependencyOnConfig} gives the configuration
variable, the corresponding lilypond macro(s) and a short description.
The dependency is not strict, because some default settings are
applied, but in general the logic described in the figure is a good
orientation.  Video voice names are not specified in a single
variable, but via video target and video file kind definitions (see
section~\ref{section:videoGeneration}).

For example, assume we have three voices in the song called
``vocals'', ``drums'' and ``guitar''. We also assume that we shall
have all voices in the midi file, vocals in an extract, drums and
guitar in the score and vocals and guitar in the video.

So the configuration file for the song contains the following
definitions:

\begin{configurationFileCode}
  ...
  voiceNameList       = "vocals, drums, guitar"
  extractVoiceNameSet = "vocals"
  scoreVoiceNameList  = "guitar, drums"
  midiVoiceNameList   = "vocals, guitar, drums"
  ...
\end{configurationFileCode}

Note that the \embeddedCode{midiVoiceNameList} could be omitted,
because the default is to use the voices from the overall voice list
\embeddedCode{voiceNameList} and the ``wrong'' order of voices does
not really matter in the midi file.  In this example the audio
variable \embeddedCode{audioVoiceNameSet} has been omitted: it
defaults to the setting of \embeddedCode{midiVoiceNameList}, so we
nevertheless have audio for ``vocals'', ``guitar'' and ``drums'' (that
means, all voices).

For the given configuration we must have the following macros in the
lilypond fragment file:
\begin{lilypondFileCode}
  keyAndTime = {...}

  vocalsExtract = {...}
  vocalsScore   = {...}
  vocalsMidi    = {...}

  guitarScore   = {...}
  guitarMidi    = {...}
  guitarVideo   = {...}

  myDrumsScore  = {...}
  myDrumsMidi   = {...}
\end{lilypondFileCode}

Again some simplification is possible: when some global macros like
\embeddedCode{guitar} is introduced, the associated variants
can be omitted.

%---------------
\section{Chords}
%---------------

Because the software is used in a band context, chord symbols may also
be used.  Chords may depend on voice and very often depend on the
processing target, because the voice formatting may be different per
target.

The configuration file variable responsible for chords is
\embeddedCode{voiceNameToChordsMap} and tells where chords are shown
and for which voices.

All voices with chords are mentioned as keys and mapped onto a slash
separated list of single character abbreviations for the targets.  We
have ``e'' for the extract, ``s'' for the score and ``v'' for the
video.  There are no chords for the midi file.

So for the configuration file line

\begin{configurationFileCode}
  voiceNameToChordsMap = "{ vocals: v/s, guitar: e }"
\end{configurationFileCode}

the chords are shown for the vocals in video and score and for guitar
in its extract.  This means the lilypond fragment file must contain
the following definitions in \embeddedCode{\macro{chordmode}}:

\begin{lilypondFileCode}
  guitarChordsExtract = {...}
  vocalsChordsScore   = {...}
  vocalsChordsVideo   = {...}
\end{lilypondFileCode}

Again there is a default: when some chord macro is missing, either the
plain chords macro for the voice or even the chords for all voices are
used.

So for example, when \embeddedCode{guitarChordsExtract} is missing,
the search is first done for \embeddedCode{guitarChords} and finally
for \embeddedCode{allChords} (the latter as a catch-all since
\embeddedCode{chords} is a keyword in lilypond).

%---------------
\section{Lyrics}
%---------------

Also lyrics may be attached to voices.  Lyrics may occur in voice
extracts, in the score and in the video.  The difference to chords is
that multiple lyrics lines (for example, for stanzas) may be attached
to a single voice, hence we need an additional count information.

It is assumed that each lyrics line is always valid for all the notes
in the voice, hence you have to provide appropriate padding (at least
leading padding).

The syntax is similar to chords, hence we have a
\embeddedCode{voiceNameToLyricsMap}, but it also contains a count of
parallel lyrics lines directly following the target letter (``e'' for
the extract, ``s'' for the score and ``v'' for the video).

So for the configuration file line

\begin{configurationFileCode}
  voiceNameToLyricsMap = "{ vocals: e2/s2/v, bgVocals: e3 }"
\end{configurationFileCode}

the lyrics are shown for the vocals in extract, video and score and
for the background vocals only in its extract.  The lyrics line macros
have capital letters as suffices (A, B, \dots) and hence are confined
to 26 parallel lines per voice.

This means the lilypond fragment file must contain the
following definitions in \embeddedCode{\macro{lyricmode}}:
\begin{lilypondFileCode}
  vocalsLyricsExtractA = {...}
  vocalsLyricsExtractB = {...}
  vocalsLyricsScoreA   = {...}
  vocalsLyricsScoreB   = {...}
  vocalsLyricsVideoA   = {...}

  bgVocalsLyricsExtractA = {...}
  bgVocalsLyricsExtractB = {...}
  bgVocalsLyricsExtractC = {...}
\end{lilypondFileCode}

Again there is a default: when some lyrics macro is missing, the macro
for the voice without the target (but with the appropriate suffix) is
used.  So for example, for a missing \embeddedCode{vocalsLyricsScoreB}
an existing \embeddedCode{vocalsLyricsB} is used.  Additionally for
the first line the suffix may be totally omitted, so
\embeddedCode{vocalsLyricsScoreA} can be replaced by
\embeddedCode{vocalsLyricsScore} or even \embeddedCode{vocalsLyrics}.

%--------------------------------------------------------
\section{Things Not to Put in the Lilypond Fragment File}
%--------------------------------------------------------

Because the different phases add their own boilerplate code, the
following lilypond code must not occur in the lilypond fragment file:

\begin{itemize}
  \item a \embeddedCode{\macro{score}} block, and
  \item staff definitions
\end{itemize}

The following should not occur in the fragment, unless you want to
override the presets from the program:

\begin{itemize}
  \item a \embeddedCode{\macro{header}} block,
  \item a \embeddedCode{\macro{paper}} block, and
  \item a setting of the \embeddedCode{global-staff-size}
\end{itemize}

Note that settings overriding presets above might interfere with some
phases: e.g. the videos use their own paper and resolution settings
and those would be shadowed by conflicting definitions in the
fragment.

%========================================
\chapter{Configuration File Settings}
\label{section:configurationFileSettings}
%========================================

In the following we show all the settings of the configuration file in
detail and what to put in an associated lilypond music fragment file.

In principle one only needs a \emph{single} configuration file and a
single lilypond fragment file.  For systematic reasons the information
can be divided for didactic reasons and must then be combined into a
single configuration file by \embeddedCode{INCLUDE} statements.

Very often reasonable defaults are used for the variables.  All
settings are described in a table in figure in
appendix~\ref{section:configurationFileVariableList}.

%------------------------------
\section{Overall Configuration}
%------------------------------

In this section the configuration file settings are discussed that
define the locations of programs and files used.  Note that paths use
the Unix forward slash as a separator.  If a relative path is used, it
is relative to the current directory where the program call is made.

Some variables define the program locations and global program
parameters and are shown in
figure~\ref{figure:configFileProgramVariables}.  For example,
\embeddedCode{ffmpegCommand} tells the path of the ffmpeg command (you
wouldn't have guessed that, would you?).

Two entries are special: \embeddedCode{aacCommandLine} and
\embeddedCode{audioProcessor}.
\begin{itemize}

  \item The aac command line specifies the complete line for an aac
        encoding command with \placeholder{infile} and
        \placeholder{outfile} as placeholders for the input and output
        file name.  If empty, ffmpeg is used for aac encoding.

  \item The audio processor map settings variable specifies three
        commands lines used in the refinement and mix phases and
        two (optional) strings used for the refinement commands:
        \begin{itemize}

          \item the command line for refining audio files with
                \placeholder{infile}, \placeholder{outfile} and
                \placeholder{effects} as placeholders for the input
                and output file name and the refinement effects
                from a sound style,

          \item the command line for mixing audio files with associated
                volume factors containing \placeholder{factor},
                \placeholder{infile} and \placeholder{outfile} as
                placeholders with the repeating group of factor and
                infile embraced by parentheses,

          \item the effect for amplifying an audio file by some factor
                given in dB containing
                \placeholder{amplificationLevel} as placeholder,

          \item the command line for padding an audio files with leading
                silence containing \placeholder{duration} (in seconds),
                \placeholder{infile} and \placeholder{outfile} as
                placeholders, and

          \item the strings for separation of parallel chains and for
                the redirection into temporary buffers (see below)

        \end{itemize}
\end{itemize}

\begin{centeredFigure}
  \begin{variableDescriptionTableLong}

    \varDescriptionHeaderLong

    \varDescriptionLong{aacCommandLine}
               {\DESCaacCommandLine}
               {"/pathto/qaac -V100 -i \placeholder{infile}
                 -o \placeholder{outfile}"}

    \varDescriptionLong{ffmpegCommand}
               {\DESCffmpegCommand}
               {"/pathto/ffmpeg"}

    \varDescriptionLong{lilypondCommand}
               {\DESClilypondCommand}
               {"/pathto/lilypond"}

    \varDescriptionLong{lilypondVersion}
               {\DESClilypondVersion}
               {"2.18.2"}

    \varDescriptionLong{midiToWavRendering\-CommandLine}
               {\DESCmidiToWavRenderingCommandLine}
               {"/pathto/fluidsynth"}

    \varDescriptionLong{mp4boxCommand}
               {\DESCmpfourboxCommand}
               {"/pathto/mp4box"}

  \end{variableDescriptionTableLong}

  \caption{Global Configuration Variables for Programs}
  \label{figure:configFileProgramVariables}
\end{centeredFigure}

\begin{centeredFigure}
  \begin{variableDescriptionTableShort}

    \varDescriptionShort{amplificationEffect}
                        {\DESCaudioProcessorAmplificationEffect}

    \varDescriptionShort{chainSeparator}
                        {\DESCaudioProcessorChainSeparator}

    \varDescriptionShort{mixingCommandLine}
                        {\DESCaudioProcessorMixingCommandLine}

    \varDescriptionShort{paddingCommandLine}
                        {\DESCaudioProcessorPaddingCommandLine}

    \varDescriptionShort{redirector}
                        {\DESCaudioProcessorRedirector}

    \varDescriptionShort{refinementCommandLine}
                        {\DESCaudioProcessorRefinementCommandLine}

  \end{variableDescriptionTableShort}

  \caption{Parameters for Command Lines in
           \embeddedCode{audioProcessor} Variable}
  \label{figure:configFileAudioProcessorVariables}
\end{centeredFigure}

So an example setting in the configuration file for the global
configuration variables could look like that:

\begin{configurationFileCode}
  aacCommandLine     = "/usr/local/qaac -V100 -i $1 -o $2"
  ffmpegCommand      = "/usr/local/ffmpeg"
  lilypondCommand    = "/usr/local/lilypond"
  lilypondVersion    = "2.18.2"
  midiToWavRenderingCommandLine =
      "/usr/local/fluidsynth ${infile} ${outfile}"
\end{configurationFileCode}

Note that \ltbvc\ tries to locate the programs on your system's
executable path.  When they can be found, \textbf{you do not have to
  specify anything here}: the defaults are used instead.

When using the standard software ``sox'' for audio refinement, this is
specified by setting the \embeddedCode{audioProcessor} variable
accordingly using the components from
figure~\ref{figure:configFileAudioProcessorVariables}.

\begin{configurationFileCode}
  _sox = "/usr/local/sox --buffer 100000 --multi-threaded"
  audioProcessor =
    "{ redirector: '->',"
    "  chainSeparator: ';',"
    "  amplificationEffect: 'gain ${amplificationLevel}',"
    "  mixingCommandLine: '" _sox
         " -m [-v ${factor} ${infile} ] ${outfile}',"
    "  paddingCommandLine: '" _sox
         " ${infile} ${outfile} pad ${duration}',"
    "  refinementCommandLine: '" _sox
         " ${infile} ${outfile} ${effects}' }"
\end{configurationFileCode}

The above setting is also the default if you do not specify the
processor.

Other variables shown in figure~\ref{figure:configFilePathVariables}
define file and path locations.  Very important is the path where the
logging file \embeddedCode{ltvbc.log} is located: sometimes it is the
only way to find out what went wrong.  But ---~as mentioned in
section~\ref{section:programUsage}~--- you can also specify the
logging file name via the command-line.

Temporary files go to \embeddedCode{intermediateFileDirectoryPath}.
By default, all temp files go to the current directory and the
phase-internal files are deleted at the end of a phase (but you can
prevent that, see chapter~\ref{section:debugging}).

\begin{centeredFigure}
  \begin{variableDescriptionTableLong}

    \varDescriptionHeaderLong

    \varDescriptionLong{intermediateFile\-DirectoryPath}
               {\DESCintermediateFileDirectoryPath}
               {"temp"}

    \varDescriptionLong{loggingFilePath}
               {\DESCloggingFilePath}
               {"/pathto/ltbvc.log"}

    \varDescriptionLong{targetDirectoryPath}
               {\DESCtargetDirectoryPath}
               {"generated"}

    \varDescriptionLong{tempAudioDirectoryPath}
               {\DESCtempAudioDirectoryPath}
               {"/pathto/audiofiles"}

    \varDescriptionLong{tempLilypondFilePath}
               {\DESCtempLilypondFilePath}
               {"temp\_\placeholder{phase}
                \_\placeholder{voiceName}.ly"}

  \end{variableDescriptionTableLong}
  \caption{Global Configuration Variables for File Paths}
  \label{figure:configFilePathVariables}
\end{centeredFigure}

An example setting in the configuration file for file path
configuration variables could look like that:

\begin{configurationFileCode}
  intermediateFileDirectoryPath = "temp"
  loggingFilePath = "/var/logs/ltbvc.log"
  targetDirectoryPath = "generated"
  tempAudioDirectoryPath = "~/ltbvc_audiofilesdir"
  tempLilypondFilePath = "temp\_\placeholder{phase}\_\placeholder{voiceName}.ly"
\end{configurationFileCode}

Note that \embeddedCode{tempLilypondFilePath} may have placeholders
for the processing phase and the voice name.  This is only relevant
when the intermediate files are kept: otherwise the temp files are
deleted after a program run.

%---------------------------------
\section{Song Group Configuration}
%---------------------------------

Very often several songs are combined into a song group, for example,
into an album.

A song group is characterized by two parameters in the configuration
file as shown in
figure~\ref{figure:configFileSongGroupRelatedVariables}.

\begin{centeredFigure}
  \begin{variableDescriptionTableLong}

    \varDescriptionHeaderLong

    \varDescriptionLong{albumName}
               {\DESCalbumName}
               {"Best of Fredo"}

    \varDescriptionLong{artistName}
               {\DESCartistName}
               {"Fredo"}

  \end{variableDescriptionTableLong}

  \caption{Song Group Related Configuration File Variables}
  \label{figure:configFileSongGroupRelatedVariables}
\end{centeredFigure}

%---------------------------
\section{Song Configuration}
%---------------------------

The song is characterized by some very simple parameters in the
configuration file shown in
figure~\ref{figure:configFileSongRelatedVariables}.  The most
important variable is \embeddedCode{fileNamePrefix} because it is used
in the file names of the generated files; all the other variables may
be missing and are set to some reasonable default.

The lilypond include file containing all fragments can be specified
via \embeddedCode{includeFilePath}, but if unset defaults to
\embeddedCode{fileNamePrefix} plus ``-music.ly''.

\begin{centeredFigure}
  \begin{variableDescriptionTableLong}

    \varDescriptionHeaderLong

    \varDescriptionLong{composerText}
               {\DESCcomposerText}
               {"arranged by Fredo, 2021"}

    \varDescriptionLong{fileNamePrefix}
               {\DESCfileNamePrefix}
               {"wonderful\_song"}

    \varDescriptionLong{includeFilePath}
               {\DESCincludeFilePath}
               {"wonderful\_song-music.ly"}

    \varDescriptionLong{intermediateFilesAreKept}
               {\DESCintermediateFilesAreKept}
               {False}

    \varDescriptionLong{measureToTempoMap}
               {\DESCmeasureToTempoMap}
               {"\{ 1 : 60|3/4, 20 : 100 \}"}

    \varDescriptionLong{trackNumber}
               {\DESCtrackNumber}
               {22}

    \varDescriptionLong{title}
               {\DESCtitle}
               {"Wonderful Song"}

    \varDescriptionLong{year}
               {\DESCyear}
               {2021}

  \end{variableDescriptionTableLong}

  \caption{Song Related Configuration File Variables}
  \label{figure:configFileSongRelatedVariables}
\end{centeredFigure}

%-----------------------------------------------
\section{Configuration of the Processing Phases}
%-----------------------------------------------

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\subsection{Preprocessing Phases}
\label{section:preprocessingPhases}
%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

All preprocessing phases rely on the configuration and the lilypond
fragment file, while the postprocessing phase start from the generated
midi file and the silent videos.

In each preprocessing phase the lilypond fragment file with the music
is embedded into some generated boilerplate lilypond file and this
file is then input for the notation typesetter
\embeddedCode{lilypond}.

\begin{centeredFigure}
  \centeredExternalPicture{0.7}{preprocessingPhaseDependencies.mps}
  \caption{Information Flow for the Preprocessing Phases}
  \label{figure:preprecessingPhases}
\end{centeredFigure}

Figure~\ref{figure:preprecessingPhases} shows the connection between
the inputs and the outputs for the phases.  Both lilypond fragment
file and configuration file serve as manual input into the processing
chain, the other files are generated.

For the ``extract'' and ``score'' phases this is all there is to do,
but the ``midi'' and ``silentvideo'' phases do further processing:
\begin{ttDescription}
  \item [midi:]
        the midi file produced by lilypond has humanization applied to
        the voices, and

  \item [silentvideo:]
        the image files produced by lilypond are combined into a
        correctly timed video and a subtitle file in SRT format is
        produced
\end{ttDescription}

If you \emph{really} want to fiddle with lilypond, the processing
phase is provided as the lilypond macro
\embeddedCode{ltbvcProcessingPhase} with values ``extract'',
``score'', ``midi'' or ``silentvideo''.  You can use that for
conditional processing, layout changes etc., because the fragment file
is included into the boilerplate file at a very late position.  Be
warned that the whole generation might fail, because the generator
assumes a simple-structured lilypond include file.

%...................................................................
\subsubsection{Notation Generation: ``extract'' and ``score'' Phase}
\label{subsubsection:notation}
%...................................................................

%````````````````````````````
\paragraph{Preliminaries}
%````````````````````````````

The central settings in the configuration file define the
characteristics of the voices.  Each voice is given by its name (an
identifier) in the variable \embeddedCode{voiceNameList}.

Note that the order in the voice name list is significant, because
later on variable in other phases rely on that order.  For example,
the reverb levels for phase ``refinedaudio'' in variable
\embeddedCode{reverbLevelList} have the same order as the
\embeddedCode{voiceNameList}.  So the lines

\begin{configurationFileCode}
  voiceNameList   = "vocals, guitar, drums"
  reverbLevelList = "   0.2,    0.3,   0.1"
\end{configurationFileCode}

associate ``vocals'' with reverb level 0.2, ``guitar'' with level 0.3
etc.  A simple table logic: and it is fine to align the data in
different entries with blanks.

The staff layout is specified by several variables that map voice
names into several kinds of staff-related layout information.  Because
this might be phase-dependent, another mapping layer is added, mapping
the phase onto the voice name to staff info map.

\embeddedCode{phaseAndVoiceNameToStaffListMap} tells the staff to use
for the voice in extract, score and video for a given processing
phase.  Default is ``Staff'', special staffs like ``DrumStaff'' may be
defined in the map.  The mapping goes from phase name to a map from
voice name to staff names.

To reduce the mental complexity we first define a map from voice name
to staff by the following configuration file lines

\begin{configurationFileCode}
  _voiceNameToStaffListMap =
      "{ drums      : DrumStaff,"
       " keyboard   : PianoStaff,"
       " percussion : DrumStaff }"
\end{configurationFileCode}

that are reused in the mapping from phase name

\begin{configurationFileCode}
  phaseAndVoiceNameToStaffListMap =
      "{ extract : " _voiceNameToStaffListMap ","
       " midi    : " _voiceNameToStaffListMap ","
       " score   : " _voiceNameToStaffListMap ","
       " video   : " _voiceNameToStaffListMap "}"
\end{configurationFileCode}

Very often the different phases use exactly identical definitions.
Hence the approach shown above is often fine (with individual
definitions per phase if necessary).  Note that only
\embeddedCode{phaseAndVoiceNameToStaffListMap} is used by the
generator, \embeddedCode{\_voiceNameToStaffListMap} is just an
auxiliary variable.

The default is ``drums'' and ``percussion'' as ``DrumStaff'' in
Lilypond, the rest uses ``Staff''.

It is also allowed to have more than one staff as the target of a
voice.  In that case the staff names are slash-separated and are
filled from several voice macros in the lilypond fragment file.  For
two systems the macros are \meta{voice}Top and \meta{voice}Bottom with
the phase target name appended, for three systems we have
\meta{voice}Top, \meta{voice}Middle and \meta{voice}Bottom.  For
example, a keyboard with a piano staff in a score references the
macros \embeddedCode{keyboardTopScore} and
\embeddedCode{keyboardBottomScore}.

Some replacement is done: if, for example, \meta{voice}MiddleExtract
does not exist, \meta{voice}Middle and finally \meta{voice} are taken
instead.

So for a guitar with a tab the following definition in the
configuration file is fine and it either reuses the
\embeddedCode{guitar} macro in the lilypond fragment file for both
staffs or you can define special guitarTop/guitarBottom macros to
differentiate:

\begin{configurationFileCode}
  ...
  "guitar"   : "Staff/TabStaff",
  ...
\end{configurationFileCode}

When reusing the same voice data in different staffs, be careful with
respect to the midi generation.  Normally you only want the voice
notes \emph{once} in the midi file, hence you will have to adapt the
\embeddedCode{phaseAndVoiceNameToStaffListMap} definition and only
include one staff in the midi file.

A similar logic as for the staffs applies to the mapping from voice
name to clef.  The standard clef is ``G'', others have to be defined
explicitely.  Especially this applies to multi-system-staffs like the
``PianoStaff'': here at least the ``xxxBottom'' must have a special
clef definition (it must be a bass clef).

A typical definition might be given as follows:

\begin{configurationFileCode}
  _voiceNameToClefMap =
      "{ bass"           : 'bass_8', "
       " drums"          : '',"
       " guitar"         : 'G_8',"
       " keyboardBottom" : 'bass',"
       " percussion"     : '' }"
\end{configurationFileCode}

Here bass and guitar have the transposed clef (as their traditional
notation), drums and percussion have none and the lower part of a
piano staff is notated in a bass clef.

Again the above is only an auxiliary definition.  The relevant
variable is \embeddedCode{phaseAndVoiceNameToClefMap} shown below.  In
our case ---~as above~--- the mapping is identical for all phases,
but, of course, individual definitions per phase are possible.

\begin{configurationFileCode}
  phaseAndVoiceNameToClefMap =
      "{ extract : " _voiceNameToClefMap ","
       " midi    : " _voiceNameToClefMap ","
       " score   : " _voiceNameToClefMap ","
       " video   : " _voiceNameToClefMap "}"
\end{configurationFileCode}

The above definition is the default, if you do not specify anything.

Figure~\ref{figure:configFileNotationVariables} shows all notation
related configuration variables discussed in the current section.

\begin{centeredFigure}
  \begin{variableDescriptionTableLong}

    \varDescriptionHeaderLong

    \varDescriptionLong{phaseAndVoiceName\-ToClefMap}
                       {\DESCphaseAndVoiceNameToClefMap}
                       {see text}

    \varDescriptionLong{phaseAndVoiceName\-ToStaffListMap}
                       {\DESCphaseAndVoiceNameToStaffListMap}
                       {see text}

    \varDescriptionLong{voiceNameToChordsMap}
                       {\DESCvoiceNameToChordsMap}
                       {"\{vocals: v/s, guitar: e\}"}

    \varDescriptionLong{voiceNameToLyricsMap}
                       {\DESCvoiceNameToLyricsMap}
                       {"\{vocals: e2/s2/v\}"}

  \end{variableDescriptionTableLong}

  \caption{Notation Generation Configuration File Variables}
  \label{figure:configFileNotationVariables}
\end{centeredFigure}

%````````````````````````````
\paragraph{``extract'' Phase}
%````````````````````````````

Once everything is set up as described above, the ``extract'' phase
generates an extract for each voice given in
\embeddedCode{extractVoiceNameSet}.  The processing order of the
voices is undefined.

As a result of this phase for each voice an extract pdf file is put
into the directory given by \embeddedCode{targetDirectoryPath} with
name \embeddedCode{fileNamePrefix}, a dash, the voice name and the
extension ``.pdf''.

The headings in the extract are set as follows: the song name from the
\embeddedCode{title} variable is the extract title, the voice name is
the extract subtitle, and the contents of \embeddedCode{composerText}
is the text for the composer part.

Figure~\ref{figure:exampleExtractLayout} shows how the first page of
an extract might look like and
figure~\ref{figure:configFileExtractVariables} shows the specific
configuration variables for voice extracts.

\begin{centeredFigure}
  \centeredExternalPicture{0.3}{demo-extract.png}
  \caption{Example Layout of an Extract File}
  \label{figure:exampleExtractLayout}
\end{centeredFigure}

\begin{centeredFigure}
  \begin{variableDescriptionTableLong}

    \varDescriptionHeaderLong

    \varDescriptionLong{extractVoiceNameSet}
                       {\DESCextractVoiceNameSet}
                       {"vocals, drums"}

  \end{variableDescriptionTableLong}

  \caption{Extract Generation Configuration File Variables}
  \label{figure:configFileExtractVariables}
\end{centeredFigure}

%``````````````````````````
\paragraph{``score'' Phase}
%``````````````````````````

In the ``score'' phase the generator produces a single score with the
voices given in \embeddedCode{scoreVoiceNameList} in the order given
by this variable and with default layout parameters.

The score pdf file is put into the directory given by
\embeddedCode{targetDirectoryPath} with name
\embeddedCode{fileNamePrefix} followed by ``\_score'' and the
extension ``.pdf''.

Headings in the score are set as follows: the song name from the
\embeddedCode{title} variable is the score title and the contents of
\embeddedCode{composerText} is the text for the composer part.

Because voice names might be long, there is a mapping that provides a
short name for each voice to be used in the score as the system
identification by filling the variable
\embeddedCode{voiceNameToScoreNameMap}.  A possible setting is:

\begin{configurationFileCode}
  voiceNameToScoreNameMap =
      "{ bass           : bs,"
       " bgVocals       : bvc,"
       " drums          : dr,"
       " guitar         : gtr,"
       " keyboard       : kb,"
       " keyboardSimple : kb,"
       " organ          : org,"
       " percussion     : prc,"
       " strings        : str,"
       " synthesizer    : syn,"
       " vocals         : voc }"
\end{configurationFileCode}

With the settings above, the ``bass'' voice has a ``bs'' name in the
score.  You do not have to use that mechanism: the default is just to
use the original voice name for staff identification in the score.

Figure~\ref{figure:exampleScoreLayout} shows how the first page of a
score might look like, figure~\ref{figure:configFileScoreVariables}
shows the specific configuration variables for scores.

\begin{centeredFigure}
  \centeredExternalPicture{0.22}{demo-score.png}
  \caption{Example Layout of a Score File}
  \label{figure:exampleScoreLayout}
\end{centeredFigure}

\begin{centeredFigure}
  \begin{variableDescriptionTableLong}

    \varDescriptionHeaderLong

    \varDescriptionLong{scoreVoiceNameList}
                       {\DESCscoreVoiceNameList}
                       {"vocals, guitar, drums"}

    \varDescriptionLong{voiceNameToScore\-NameMap}
                       {\DESCvoiceNameToScoreNameMap}
                       {"\{ vocals : voc, bass : bs \}"}

  \end{variableDescriptionTableLong}

  \caption{Score Generation Configuration File Variables}
  \label{figure:configFileScoreVariables}
\end{centeredFigure}

%...................................................
\subsubsection{Midi File Generation: ``midi'' Phase}
\label{section:midiPhase}
%...................................................

The lilypond fragment file normally does not contain any further
macros for MIDI because the voices used for the score are often fine
for the MIDI file.

Nevertheless it could happen that you need special processing here.
Examples are
\begin{itemize}

  \item A voice has different notes or is transposed in the MIDI and
        audio rendering than in the notation. This can be achieved by
        having a different \embeddedCode{\meta{voice}Midi} macro.

  \item Some hidden voice occurs in MIDI and audio output, for example,
        a voice delayed or transposed relative to some other voice (to
        enhance the sound of the original voice).  This can be
        achieved by adding a voice to the \embeddedCode{voiceNameList}
        macro, but excluding it from extracts, score and video.
\end{itemize}

The ``midi'' processing phase unfolds all repeats in the given voices
and generates corresponding midi streams.  Those streams are generated
only for those voices specified in the configuration variable
\embeddedCode{midiVoiceNameList} and stored in a single file in the
directory given by \embeddedCode{targetDirectoryPath} with name
\embeddedCode{fileNamePrefix} plus ``-std'' and extension ``.mid''.

\begin{centeredFigure}
  \begin{variableDescriptionTableLong}

    \varDescriptionHeaderLong

    \varDescriptionLong{midiVoiceNameList}
               {\DESCmidiVoiceNameList}
               {"guitar, drums"}

    \varDescriptionLong{midiChannelList}
               {\DESCmidiChannelList}
               {see text}

    \varDescriptionLong{midiInstrumentList}
               {\DESCmidiInstrumentList}
               {see text}

    \varDescriptionLong{midiVolumeList}
               {\DESCmidiVolumeList}
               {see text}

    \varDescriptionLong{panPositionList}
               {\DESCpanPositionList}
               {see text}
  \end{variableDescriptionTableLong}

  \caption{Midi Related Configuration File Variables}
  \label{figure:configFileMidiRelatedVariables}
\end{centeredFigure}

All those voices have specific settings defined by several list
variables, that align with the list \embeddedCode{voiceNameList} and
are shown in figure~\ref{figure:configFileMidiRelatedVariables}.

For example, the following settings in the configuration file

\begin{configurationFileCode}
  voiceNameList      = "vocals, guitar, drums"
  midiChannelList    = "   1,      2,     10 "
  midiInstrumentList = "  54,   2:29,     16 "
  midiVolumeList     = "  90,     60,    110 "
  panPositionList    = "   C,   0.5L,    0.1R"
\end{configurationFileCode}

define vocals to be a synth vox in the center with 3/4 volume, the
guitar to be an overdrive guitar (in bank 2), located half left with
medium volume, and the drums to be a power set, located slightly right
with almost full volume.

Nevertheless the midi phase not only transforms lilypond to plain
midi, but does further processing by adding \emph{humanization}.  This
is specified by the variable \embeddedCode{humanizedVoiceNameSet}: it
tells what voices shall be humanized, the others are left untouched.

Humanization is done by adding random variations in timing and
velocity to the notes in a voice.  This is not completely random, but
depends on voice, position within measure and on the style of the
song.

The voice- (or instrument-specific) variation is global and defined by
the configuration variable
\embeddedCode{voiceNameToVariationFactorMap}.  Each voice name is
mapped onto a slash-separated pair of two numbers with the first
giving the velocity, the second the timing variation percentage.

For a standard band instrument set, we take the variations of the drum
as the reference in a humanization style.  Hence drums should have an
instrument-specific variation factor of 1.0 each which means that the
calculated variation for some note is taken directly for drums.  Other
voices like, for example, vocals are slightly more loose and might
have a value of 1.5 for velocity and 1.2 for timing which means that
the calculated variation for those parameters is scaled accordingly.
Of course, the velocity values are adjusted to their ranges after the
variation, because there is a maximum and minimum velocity.

Our example would result in

\begin{configurationFileCode}
  voiceNameToVariationFactorMap = "{ drums: 1.0/1.0,"
                                   " vocals: 1.5/1.2}"
\end{configurationFileCode}

The \emph{humanization style} of a song tells individual variations
based on the position of a note within a measure.  Hence it gives
timing and velocity variations for the main beats and all other notes.

A \emph{timing variation} is a positive decimal number and tells how
much a note can be shifted in \(1/32^{nd}\) notes (where 0 means no
shift at all, 1 means a shift by at most a \(1/32^{nd}\) etc.).  A
\emph{velocity variation} tells the standard velocity level of a note
at this position and the slack gives the maximum variation.

When specifying a style, the note positions within a measure are given
as decimal fractions of a semibreve giving the offset to the measure
start.  For example, the first beat in a measure has offset 0, the
third beat an offset of 0.5.  Additionally each style specifies a
raster size \(r\), for example 0.125 for an eight note raster.  When a
measure position is given by an offset \(o\), all notes in the open
interval \((o-\frac{r}{2}, o+\frac{r}{2})\) will be handled by the
given humanization definition.

The algorithmic logic for a note humanization is as follows:
\begin{enumerate}

  \item Assume that the given note has time \(t_i\) and velocity
        \(v_i\). Further assume that length of a thirtysecond
        note in time units is \(\ell\) and that the instrument-specific
        adjustments from the table are \(adj_t\) and \(adj_v\).

  \item Pick two random numbers \(r_t\) and \(r_v\) both in
        the interval \([-1,1]\) from a quadratic probability
        distribution (which favours smaller numbers).

  \item Depending on \(t_i\) find the note position \(p_i\) within its
        measure.  Calculate the note offset within the measure and
        convert it to a fraction of a semibreve giving \(o_i\).  If
        \(o_i\) lies in some interval \((p-\frac{r}{2},
        p+\frac{r}{2})\) ---~where \(r\) is the raster size specified
        in the style~---, then the position \(p_i\) is given as p,
        otherwise the position is ``OTHER''.

  \item For the timing take the offset \(\tau(p_i)\) given by the timing
        map for the current position \(p_i\) and multiply it by
        \(r_t\) and by the length of a thirtysecond note and by the
        instrument-specific adjustment \(adj_t\) giving \(\Delta_t\).
        If the offset has a ``B''(ehind) prefix, take the absolute
        value \(abs(r_t)\) instead of \(r_t\), because the note may
        only be behind the position; if the offset has an ``A''(head)
        prefix, take the negative absolute value \(-abs(r_t)\) instead
        of \(r_t\) because the note may only be ahead of the position;
        otherwise keep the sign of \(r_t\).

        Finally we have
            \[t^\prime_i := t_i + \Delta_t
                = t_i + r_t\cdot(\tau(p_i)\cdot adj_t\cdot\ell)\]
        For a single voice the timing of notes in a voice starting
        simultaneously is changed in an identical fashion (the timing
        adjustment is ``cached'').

  \item For the velocity take the associated velocity emphasis value
        \(\sigma(p_i)\) given by the velocity map for the current
        position and the global slack in the velocity map \(\psi\).
        The velocity is first scaled by the emphasis value
        \(\sigma(p_i)\) (to accentuate beats) then randomly adjusted
        by slack \(\psi\) and instrument-specific adjustment \(adj_v\)
        and finally capped to the MIDI velocity interval \([0,
        127]\).  Note that there is no sign change on the random
        factor for the velocity.

        Finally we have
            \[v^\prime_i :=
              min(127,
                  max(0,
                      v_i \cdot (\sigma(p_i) + r_v \cdot(\psi\cdot adj_v))))\]

        If the velocity already varies within a measure, emphasis
        will \emph{not} be applied, but only the slack.  This means
        that whenever the voice already has some nontrivial
        accentuation, only some random velocity variation is applied.

\end{enumerate}

\begin{centeredFigure}
  \centeredExternalPicture{0.75}{humanization.mps}
  \caption{Automatic Humanization of a Note}
  \label{figure:humanization}
\end{centeredFigure}

Figure~\ref{figure:humanization} shows the example humanization of a
single note by the above algorithm:

\begin{itemize}
  \item Here \(p_i\) is the second quarter position and we assume that
        there is a definition available in the map for position ``0.25''.

  \item The timing for a note at that position in the measure is
        adjusted by a random offset in the interval of
        \(\pm\tau(p_i)\cdot adj_t\cdot\ell\) around the original note
        start position \(t_i\).  Here \(\tau(p_i)\) is the
        position-dependent timing factor, \(adj_t\) the
        instrument-specific timing scaling factor and \(\ell\) the
        duration of a \(1/32^{nd}\) note.

  \item The velocity for a note at that position in the measure is
        adjusted by a random offset in the interval of \(\pm\psi\cdot
        adj_v\) around the original note velocity multiplied by
        \(\sigma(p_i)\), the position-dependent velocity factor.  Here
        \(\psi\) is the position-independent slack and \(adj_v\) the
        instrument-specific velocity scaling factor.

  \item Both variations use a quadratic random distribution, which
        is symbolized by the colored parabolas in the diagram.
\end{itemize}

The idea behind the approach for the velocity is to accent some beats
in a measure.  For example, a rock style would favour the 2 and 4, a
march the 1.  Timing may be varied or even be dragged or hurried.

So altogether a single style definition is a map telling about the
velocity and the timing for positions in a measure plus information
about position raster and velocity slack.

Let us take a rock style with steady beats on two and four (so no time
variation here) and some emphasis on the second beat.  In the
configuration file it might look like

\begin{configurationFileCode}
  humanizationStyleRockHard  =
      "{ 0.00:    1/0.2, 0.25: 1.15/0,"
      "  0.50: 0.95/0.2, 0.75:  1.1/0,"
      "  OTHER: 0.9/B0.25,"
      "  RASTER : 0.03125, SLACK : 0.1 }"
\end{configurationFileCode}

All available humanization styles in the configuration file must have
a fixed prefix \embeddedCode{humanizationStyle} in their names to be
elegible.

Note that because all those definitions go anywhere in the
configuration files, humanization styles could even be song-specific.
On the other hand it is helpful to just reuse those styles, because
humanization normally should not depend on the song, but on the style
of the song only.

The song itself defines the styles to be applied as a style map from
measure number to style starting here.  Styles apply to all humanized
instruments simultaneously, it is not possible to have, for example, a
reggae on drums against a rumba on bass.

So the style map in the configuration file might look like

\begin{configurationFileCode}
  measureToHumanizationStyleNameMap =
  "{   1 : humanizationStyleRockHard,"
    " 45 : humanizationStyleBeat}"
\end{configurationFileCode}

and tells that the ``rock hard'' style defined above is used at the
beginning and that the style switches to a ``beat'' style in measure
45.

All humanization variables discussed above are shown summarized in the
table in figure~\ref{figure:configFileHumanizationVariables}.

\begin{centeredFigure}
  \begin{variableDescriptionTableLong}

    \varDescriptionHeaderLong

    \varDescriptionLong{countInMeasureCount}
                       {\DESCcountInMeasureCount}
                       {2}

    \varDescriptionLong{humanizedVoiceNameSet}
                       {\DESChumanizedVoiceNameSet}
                       {"vocals, drums, keyboard"}

    \varDescriptionLong{measureToHumaniza\-tionStyleNameMap}
                       {\DESCmeasureToHumanizationStyleNameMap}
                       {"{ 1: styleXXX, 5: styleYYY }"}

    \varDescriptionLong{humanizationStyle\-\meta{name}}
                       {\DESChumanizationStyleXXX}
                       {see text}

    \varDescriptionLong{voiceNameToVaria\-tionFactorMap}
                       {\DESCvoiceNameToVariationFactorMap}
                       {see text}
  \end{variableDescriptionTableLong}

  \caption{Midi Humanization Related Configuration File Variables}
  \label{figure:configFileHumanizationVariables}
\end{centeredFigure}

%......................................................
\subsubsection{Video Generation: ``silentvideo'' Phase}
\label{section:videoGeneration}
%......................................................

The video from the lilypond fragment file is produced by combining
rendered images from lilypond in an intelligent fashion.
``silentvideo'' just renders the video without sound, later on the
``finalvideo'' phase in the postprocessing combines the silent video
with the rendered audio tracks.

For the video rendering we need the characteristics of the video
target, for example, the size and resolution of the device used.
Additionally there is data as the rendering directory or the suffix
used for the video files.

Because it might happen that several video renderings have similar
video target properties, the information is split: a video rendering
relies on a specific video target and gives details such as the
directory where the video file goes or the names of the displayed
voices.

\begin{centeredFigure}
  \begin{variableDescriptionTableShort}

    \varDescriptionShort{height}
                        {\DESCvideoTargetHeight}

    \varDescriptionShort{width}
                        {\DESCvideoTargetWidth}

    \varDescriptionShort{resolution}
                        {\DESCvideoTargetResolution}

    \varDescriptionShort{topBottomMargin}
                        {\DESCvideoTargetTopBottomMargin}

    \varDescriptionShort{leftRightMargin}
                        {\DESCvideoTargetLeftRightMargin}

    \varDescriptionShort{systemSize}
                        {\DESCvideoTargetSystemSize}

    \varDescriptionShort{scalingFactor}
                        {\DESCvideoTargetScalingFactor}

    \varDescriptionShort{frameRate}
                        {\DESCvideoTargetFrameRate}

    \varDescriptionShort{ffmpegPresetName}
                        {\DESCvideoTargetFFMpegPresetName}

    \varDescriptionShort{mediaType}
                        {\DESCvideoTargetMediaType}

    \varDescriptionShort{subtitleColor}
                        {\DESCvideoTargetSubtitleColor}

    \varDescriptionShort{subtitleFontSize}
                        {\DESCvideoTargetSubtitleFontSize}

    \varDescriptionShort{subtitlesAreHardcoded}
                        {\DESCvideoTargetSubtitlesAreHardcoded}
  \end{variableDescriptionTableShort}

  \caption{Parameters for Video Target in
          \embeddedCode{videoTargetMap} Variable}
  \label{figure:configFileVideoTargetVariables}
\end{centeredFigure}

\begin{centeredFigure}
  \centeredExternalPicture{1.5}{videoTargetDimensions.mps}
  \caption{Target Parameters for Video Generation}
  \label{figure:targetVideoVariables}
\end{centeredFigure}

So we have two configuration file variables:
\begin{itemize}

  \item \embeddedCode{videoTargetMap} provides video device dependent
        properties of notation videos, but also some device
        independent parameters (like, for example, the subtitle font
        size).

        This variable is a map from ``target name'' to a target
        descriptor.  A target descriptor is itself a map with the
        several fields as shown in
        figure~\ref{figure:configFileVideoTargetVariables}.  Some of
        the variables like \embeddedCode{resolution},
        \embeddedCode{height} or \embeddedCode{width} describe
        ``hardware'' parameters (because normally the video should
        have the appropriate size), others like
        \embeddedCode{topBottomMargin} the layout of the video.

        Figure~\ref{figure:targetVideoVariables} shows how some of the
        parameters for video generation are connected to the physical
        output device and the video target in general.

  \item \embeddedCode{videoFileKindMap} provides further details on
        the rendering (like, for example, the list of voices to be
        shown).

        This variable is a map from a ``video file kind name'' to a
        video file kind descriptor.  A video file kind descriptor is
        itself a map with the several fields as shown in
        figure~\ref{figure:configFileVideoFileKindVariables}.  There
        is information about the target file given by
        \embeddedCode{videoDirectoryPath} and
        \embeddedCode{fileNameSuffix} and the list of the voices in
        those video files.

        \begin{centeredFigure}
          \begin{variableDescriptionTableShort}

            \varDescriptionShort{target}
                                {\DESCvideoFileKindTarget}

            \varDescriptionShort{directoryPath}
                                {\DESCvideoFileKindDirectoryPath}

            \varDescriptionShort{fileNameSuffix}
                                {\DESCvideoFileKindFileNameSuffix}

            \varDescriptionShort{voiceNameList}
                                {\DESCvideoFileKindVoiceNameList}
          \end{variableDescriptionTableShort}


          \caption{Parameters for Video File Kind in
                   \embeddedCode{videoFileKindMap} Variable}
          \label{figure:configFileVideoFileKindVariables}
        \end{centeredFigure}
\end{itemize}

So a video target definition for a single midrange tablet
could look like this:
\begin{configurationFileCode}
  videoTargetMap =
    "{"
      " tablet:"
        " { fileNameSuffix:           '-i-v',"
          " targetVideoDirectoryPath: '/pathto/tablet',"
          " resolution:               132,"
          " height:                   1024,"
          " width:                    768,"
          " topBottomMargin:          5,"
          " leftRightMargin:          10,"
          " systemSize:               25,"
          " ffmpegPresetName:         'mydevice',"
          " scalingFactor:            4,"
          " frameRate:                10,"
          " mediaType:                'TV Show',"
          " subtitleColor:            2281766911,"
          " subtitleFontSize:         20,"
          " subtitlesAreHardcoded:    false }"
    "}"
\end{configurationFileCode}

The above defines a target called ``tablet'' having a video with
1024x768~pixels, a resolution of 132dpi, a margin of 5mm at top and
bottom, a margin of 10mm left and right, slightly enlarged systems
(lilypond standard system size is 20), a yellow semi-transparent
subtitle with size 20~pixels.  The video is encoded by ffmpeg with an
ffmpeg preset called ``mydevice'' at a frame rate of 10fps (which is
ample for a more or less static video and ensures that the time
resolution for page turning and subtitle changes is 0.1s) and lilypond
produces images 4~times wider and higher than needed to be downscaled
by the video renderer for better video image quality.  The quicktime
media type is ``TV Show'' and subtitles in the final video are on a
separate track.

Based on the video target definition given above a video file kind
definition could look like this:
\begin{configurationFileCode}
  videoFileKindMap =
    "{"
      " tabletVocGtr:"
        " { target:          tablet,"
          " fileNameSuffix:  '-i-v',"
          " directoryPath:   '/pathto/xyz',"
          " voiceNameList:   'vocals, guitar' }"
    "}"
\end{configurationFileCode}

The above defines a single file kind for output.  The target
characteristics are those of a ``tablet'', those videos contain a
score with vocals plus guitar and all the files have suffix '-i-v'
(followed by '.mp4', of course).

So the silent video generation produces an MP4 video file for each
video file kind specified.  Each video displays a score with all
voices specified in the configuration variable
\embeddedCode{videoFileKind.voiceNameList} with automatic page turning
at the right points in time.  That video is stored in a single file in
the directory given by \embeddedCode{videoFileKind.directoryPath} with
name \embeddedCode{fileNamePrefix} plus "\_noaudio" and the
\embeddedCode{videoFileKind.fileNameSuffix} from the file kind
specification and extension ``.mp4''.

Additionally a subtitle file with all measure numbers is generated in
the directory given by \embeddedCode{targetDirectoryPath} with name
\embeddedCode{fileNamePrefix} plus "\_subtitle" and extension
``.srt''.

This means that a song with file name prefix ``wonderful\_song'' and a
target file name suffix ``-tablet'' leads to a silent video file of
``wonderful\_song\_no\-audio-tablet.mp4'' and a subtitle file of
``wonderful\_song\_subtitle.srt''.  Note that the subtitle file is
independent of the video target, because it only gives the time
intervals of each measure and those do not depend on the video.

If you \emph{really} want to fiddle with the video generation, the
video target name is provided as the lilypond macro
\embeddedCode{ltbvcVideoTargetName} and has the values specified as
keys in the list \embeddedCode{videoTargetMap}.  You can use this for
conditional processing, video layout changes etc., because the file
inclusion into the boilerplate file is done at a very late position.
Be warned that the whole video generation might fail, because the
generator assumes that it has to handle a simple-structured lilypond
include file.

There is only a single configuration file variable for video as shown
in figure~\ref{figure:configFileVideoVariables} that defines all video
targets that are used in the generation.

\begin{centeredFigure}
  \begin{variableDescriptionTableLong}

    \varDescriptionHeaderLong

    \varDescriptionLong{videoTargetMap}
                       {\DESCvideoTargetMap}
                       {see text}

    \varDescriptionLong{videoFileKindMap}
                       {\DESCvideoFileKindMap}
                       {see text}

  \end{variableDescriptionTableLong}

  \caption{Video Configuration File Variables}
  \label{figure:configFileVideoVariables}
\end{centeredFigure}

Because the algorithm for finding the page breaks in the video relies
on data scraping of a postscript file produced by lilypond, some
restrictions apply for the notation videos: the bar numbers are
activated for the line starts only and those bar numbers as well as
the bar lines will be black.

%````````````````````````````
\paragraph{Still Image Video}
%````````````````````````````

The \ltbvc\ can also produce a special video file just consisting of
still notation image pages, the subtitle data containing the measure
timings and information at which measure some image is shown.  This is
a non-standard video kind: it is simply a tar file with still images,
subtitle file and file with mapping from measures to image names.  Its
target must have a \embeddedCode{frameRate} of zero, because this
should not occur for real videos.

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\subsection{Postprocessing Phases}
%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

All postprocessing phases rely on the configuration file, the
generated midi file and the silent videos; the lilypond fragment file
is not used any longer.

Figure~\ref{figure:postprecessingPhases} shows the connection between
the inputs and the outputs for the phases.  Only the configuration
file serves as manual input into the processing chain, the other files
are generated from files coming from the preprocessing phases in
section~\ref{section:preprocessingPhases}.

The following processing is done:
\begin{ttDescription}
  \item [rawaudio:]
        the midi file is rendered via \embeddedCode{fluidsynth} and
        sound fonts into plain audio files for each relevant audio
        voice,

  \item [refinedaudio:]
        based on voice-specific sound definitions each plain audio
        file is refined typically by \embeddedCode{sox} processing for
        each relevant audio voice into a refined audio file,

  \item [mix:]
        mixed and mastered versions of the voice audio files are
        generated, mastered and grouped into audio groups from the
        configuration file (for later selection as audio track)
        typically by \embeddedCode{sox} , and

  \item [finalvideo:]
        the still videos and the subtitle file produced from the
        lilypond fragment file are combined with the grouped audio
        files to video files with selectable audio tracks and either
        selectable or burnt in

\end{ttDescription}

\begin{centeredFigure}
  \centeredExternalPicture{0.7}{postprocessingPhaseDependencies.mps}
  \caption{Information Flow for the Postprocessing Phases}
  \label{figure:postprecessingPhases}
\end{centeredFigure}

%........................................................................
\subsubsection{Audio Generation: ``rawaudio'' and ``refinedaudio'' Phase}
%........................................................................

Each voice in \embeddedCode{audioVoiceNameSet} is rendered to audio
files via the phases ``rawaudio'' and ``refinedaudio''. Central input
is the humanized midi file from section~\ref{section:midiPhase}.  The
\embeddedCode{audioVoiceNameSet} variable is an (unordered) list of
voices names that are a subset of those occuring in the
\embeddedCode{midiVoiceNameList}.

%`````````````````````````````
\paragraph{``rawaudio'' Phase}
%`````````````````````````````

The ``rawaudio'' phase simply takes each voice given in the audio
voice name set and converts the humanized midi stream into a wave file
using \embeddedCode{midiToWavRenderingCommandLine} typically using the
fluidsynth program.  This command line relies on soundfont files
specified in that string.  The name order of the soundfonts (of type
sf2 or sf3) give the order of matching a given midi instrument number:
the first match is accepted.

Note that the midi volume is not used by this phase: any midi volume
changes are suppressed and only the velocity is used.

For each voice the resulting wave file after generation is stored in
directory \embeddedCode{tempAudioDirectoryPath} as an intermediate
file for further processing.  The naming convention is to use the
voice name with a ``.wav'' extension (for example, ``bass.wav'' stores
the result for a bass voice).

%`````````````````````````````````
\paragraph{``refinedaudio'' Phase}
%`````````````````````````````````

Normally the sounds produced by soundfonts need some beefing up.  This
is done in the ``refinedaudio'' phase where the audio file from the
previous phase are postprocessed by the sound processor
\embeddedCode{sox}.

sox is a commandline program where chains of effects are applied to
audio input files producing audio output files.  For example, the
command
\begin{commandLine}
  sox input.wav output.wav highpass 80 2q reverb 50
\end{commandLine}
applies a double-pole highpass filter at 80Hz with a width of 2q
followed by a medium reverb to file \embeddedCode{input.wav} and
stores the result in file \embeddedCode{output.wav}.

sox has a lot of those filters and all those can be used for sound
shaping.  In this document we cannot go into details, but a thorough
information can be found in the sox
documentation~\cite{reference:soxDocumentation}.

Of course, it is also possible to use another command-line audio
processor by setting the variable \embeddedCode{audioProcessor}
appropriately and adapting the refinement commands for the voices for
the tool used.  But this is an expert solution beyond the scope of
this documentation; hence you are on your own\dots

Each audio voice is transformed depending on voice-specific settings
in the configuration file.  Because the input file comes from the
previous ``rawaudio'' phase (for example ``bass.wav'') and the output
file name for the ``refinedaudio'' phase is also well-defined (for
example as ``bass-processed.wav''), we only have to specify the sox
effects for the transformation itself.

Those effects depend on the voice/instrument and on the style of the
playing and this is combined in a so-called \emph{sound style}
variable.

The name of sound style variables is constructed as follows: the
prefix ``soundStyle'' is followed by the voice name with initial caps
(for example ``Bass'') and by the style variant ---~a single word~---
capitalized as suffix (``Hard'').  When following this convention, a
hard bass has a sound style name ``soundStyleBassHard''.

Very often a sound style is not defined on its own, but relies on
other definitions.  Let us assume we have some standard postprocessing
for a bass.  This consists of a normalization with 24dB headroom (to
prevent distortion in the following steps), an enhancement of the
150Hz band by 10dB and a 6dB cutoff of high frequencies above 600Hz.
In the configuration file this could look as follows:

\begin{configurationFileCode}
  _bassPostprocess =
      " norm -24"
      " equalizer  150 4o +10"
      " lowpass -2 600 1.2o"
\end{configurationFileCode}

Based on that definition above the actual sound style can be defined
as follows (referencing the definition by name):

\begin{configurationFileCode}
  soundStyleBassHard =
      " highpass -2 40"
      " lowpass -2 2k"
      " norm -6 "
      " tee"
      " overdrive 12 0 "
      _bassPostprocess
\end{configurationFileCode}

The sound style definition uses a low- and highpass followed by an
overdrive and the final equalization. Note that the name is \emph{not}
in double quotes: this distinguishes it from plain text (as explained
in section~\ref{section:configurationFileSyntax}).

There are four things to note:
\begin{enumerate}

  \item As demonstrated sound styles may rely on other definitions; so
        you can build a hierarchy of effect chains.

  \item The special effect ``tee'' is not part of sox.  When debugging
        is active, this ``effect'' writes out the audio data available
        at that position in the chain into a temporary file in the
        target audio directory called ``\meta{voice}X.wav'' where X
        stands for a hex number.  Multiple ``tee'' occurrences are
        possible, so you can do an audio debugging of your chain.

  \item Normally processing is purely sequential with a single signal
        path (which is standard sox behaviour).  But it is possible to
        add parallel signal paths and to combine them (e.g. for New
        York parallel compression etc.).  See below\dots

  \item Reverb may be specified in the chain or ---~for really
        simple applications~--- is automatically applied with default
        parameters and an intensity defined by the configuration
        variable \embeddedCode{reverbLevelList} to the final audio.

        Note that this feature is only available when the sox audio
        processor is used.

        If this is not the case or the simple reverb is not good
        enough and specific settings are needed, you can set the
        reverb level for some voice to 0 and add a more elaborate
        reverb effect to the sound style.  If you leave off the
        \embeddedCode{reverbLevelList} altogether, all voices have no
        automatic reverb applied.

\end{enumerate}

So how do we apply the specific sound style and some reverb to our
bass?  The settings in the song configuration file are as follows

\begin{configurationFileCode}
  voiceNameList    = "...,  bass, ..."
  reverbLevelList  = "...,   0.4, ..."
  soundVariantList = "...,  HARD, ..."
\end{configurationFileCode}

As above \embeddedCode{reverbLevelList} and
\embeddedCode{soundVariantList} are lists with elements in the same
order as \embeddedCode{voiceNameList}.  There is a special sound
variant called \embeddedCode{copy} that just takes the raw audio file
and applies the specified reverb to it.  This is also the default,
when you do not specify a \embeddedCode{soundVariantList} at all.

The sound variant may be given in any letter case, because it is
automatically adapted for the selection of the sound style.  Combined
with the above sound style this leads to the following sox effects
---~when debugging is active~--- (note the effect line split at the
tee effect and the added final reverb with \embeddedCode{100\(\cdot\)
reverbLevel}):

\begin{commandLine}
  sox bass.wav bassA.wav highpass -2 40 lowpass -2 2k norm -6
  sox bassA.wav bass-processed.wav overdrive 12 0 norm -24
       equalizer 150 4o +10 lowpass -2 600 1.2o reverb 40
\end{commandLine}

In general, sound styles can be defined per song or globally.  I
prefer the latter, because I use a few bread-and-butter sounds per
instrument and adapt them only by using different midi instruments,
audio volumes and reverb levels in the voice configuration; hence the
sound styles itself are not adapted.  But in principle you can
fine-tune the voice sounds per song, which I find tedious, but
occasionally do that for fine-tuning.

For the bread-and-butter sound approach, it is helpful to use a simple
set of variant names that apply to all voices, for example, ``STD''
(for a normal sound), ``HARD'' (for some heavier sound), ``EXTREME''
(for an ultra-hard sound) etc.

So finally each audio voice has its processed wav version in
\embeddedCode{targetDirectoryPath} called
``\meta{voice}-processed.wav'' for later mixdown.

%............................
\subparagraph{Parallel Paths}
%............................

Parallel signal paths cannot be handled directly by sox and are
emulated by \ltbvc.  They can be specified as follows:
\begin{itemize}

  \item Parallel \emph{chains} are specified by using chain separators
        in the list of effects using the character token ``;'' (which
        can be redefined by setting the variable
        \embeddedCode{audioProcessor.chainSeparator}).

  \item For each chain its (single) source and target are each given
        by an identifier that is immediately preceeded or followed by
        ``->'' (which can be redefined by setting
        \embeddedCode{audioProcessor.redirector}).  So a chain target
        might be specified as ``->xxx'', a chain source might be
        specified as ``yyy->''.  When no identifier is given for a
        source, the raw audio file is used.

        Note that, of course, the name of a chain source must occur
        as a chain target somewhere before.

        Each chain has ``->'' (the raw audio file) as its implicit
        chain source, the last chain has the refined audio file as its
        implicit target.

  \item A chain may consist of a special ``mix'' effect that does a
        decibel-weighted mix of several sources into a single target.
        E.g. the chain
        \begin{configurationFileCode}
          mix 0 -> -3 A-> -6 B-> ->C
        \end{configurationFileCode}

        mixes the raw audio file with 0dB attenuation (unchanged), the
        result of chain A with -3dB attenuation (about 71\% volume)
        and the result of chain B with -6dB attenuation (about 50\%
        volume) into chain target C.

        Very often, the last chain is a mix of several sources into
        the refined audio file as the target.

        A ``mix'' effect must not have an embedded ``tee''.
\end{itemize}

As an example let us enhance a bass part by adding a copy pitched down
by an octave and having some parallel compression added.  We assume
that the bass is pre-processed by ``soundStyleBassStd'' and we simple
add the postprocessing as follows:
\begin{configurationFileCode}
  soundStyleBassStd ->A
  ; A-> pitch -1200 ->B
  ; mix 0 A-> -3 B-> ->C
  ; C-> compand 0.04,0.5 6:-25,-20,-5 -6 -90 0.02 ->D
  ; mix 0 C-> -8 D->
\end{configurationFileCode}

``A'' contains the preprocessed audio, ``B'' the pitched down version,
``C'' the enriched bass sound, ``D'' the compressed version of it and
the combined audio goes to the refined audio file.

%............................
\subparagraph{Special Tracks}
%............................

Another helpful feature of the ``refinedaudio'' phase is the ability
to introduce other audio files into the processing.  There are two
cases:

\begin{enumerate}
  \item One can override a processed track by some external audio
        file.
  \item A parallel track in a file not related to some voice can
        be added.
\end{enumerate}

So both cases involve external audio files to be added.

The first case is common when you want to replace a track by a real
recording.  For example, the vocals with midi beeps could be enhanced
by having a real singer sing the track.

All those tracks are mentioned and overridden in the configuration
variable \embeddedCode{voiceNameToOverrideFileNameMap}.  As its name
tells, it maps voice names to file names.

\begin{configurationFileCode}
  voiceNameToOverrideFileNameMap =
      "{ vocals : 'vocals.flac',"
        "bass   : 'mybass.wav'  }"
\end{configurationFileCode}

This approach replaces the processed voice files by the contents of
the files given in the map.  File types supported are all those
supported by sox as input.  Note that the overriding file has to have
the length of a refined voice file, that means, it also has to contain
material for the count-in measures.

In the second case no specific voice track is replaced, but some
parallel track is introduced.  For example, this could be used for
lead-in text or audience audio.

In principle this could be handled by introducing an artificial voice
only used for audio, but for convenience there is another variable
called \embeddedCode{parallelTrack} for a single additional track.  It
contains comma-separated data for an audio file name, a volume factor
in decibels and offset relative to the start of the song in seconds as
follows:

\begin{configurationFileCode}
  parallelTrack = " parallelFile.wav, -2, 2.8"
\end{configurationFileCode}

Note that it is only possible to have a single parallel track.

%```````````````````````````````````````````````````
\paragraph{Summary of Audio Configuration Variables}
%```````````````````````````````````````````````````

Figure~\ref{figure:configFileAudioVariables} shows all the
configuration variables described for the ``rawaudio'' and
``refinedaudio'' phases.

\begin{centeredFigure}
  \begin{variableDescriptionTableLong}

    \varDescriptionHeaderLong

    \varDescriptionLong{audioVoiceNameSet}
                       {\DESCaudioVoiceNameSet}
                       {"vocals, drums, bass"}

    \varDescriptionLong{parallelTrack}
                       {\DESCparallelTrack}
                       {"prerendered.wav, 0, 2.5"}

    \varDescriptionLong{reverbLevelList}
                       {\DESCreverbLevelList}
                       {"0.1, 1.1, 0.5, 0.0"}

    \varDescriptionLong{soundStyle\meta{Voice}\-\meta{Variant}}
                       {\DESCsoundStyleXXX}
                       {see text}

    \varDescriptionLong{soundVariantList}
                       {\DESCsoundVariantList}
                       {"COPY, EXTREME, STD, HARD"}

    \varDescriptionLong{voiceNameToOverride\-FileNameMap}
                       {\DESCvoiceNameToOverrideFileNameMap}
                       {see text}

  \end{variableDescriptionTableLong}

  \caption{Audio Configuration File Variables}
  \label{figure:configFileAudioVariables}
\end{centeredFigure}

%........................................................
\subsubsection{Final Audio Generation: ``mix'' Phase}
%........................................................

The ``mix'' phase combines the refined audio files into one or
more audio file with all voices and in aac audio format.

Audio levels and pan positions of the individual voices, mastering
effects and a final amplification factor are specified in the
configuration.  Hence the audio voices are mixed with those levels and
to the given pan positions, have the mastering audio processing
applied and finally are amplified by the given factor before the
result is compressed into an AAC file.

When the variable \embeddedCode{mixingCommandLine} does not specify a
``pan'' placeholder, panning is done internally.  This algorithm does
a traditional balancing of the stereo channels.  That means, when the
pan value is less than zero the left channel is unchanged, while the
right channel is linearly attenuated and vice versa for a positive pan
value. So the amplification factors are

\newcommand{\amplificationFunction}[3]{
    \begin{displaymath}
      {\rm amplification~factor}_{\rm #1} =
      \left \{
            \begin{array}{cl}
                #2 & {{\bf if}~\rm panValue}   <  0\\
                #3 & {{\bf if}~\rm panValue} \geq 0\\
            \end{array}
       \right .\\
    \end{displaymath}
}

\amplificationFunction{left}{1}{1 - {\rm panValue}}
\amplificationFunction{right}{1 + {\rm panValue}}{1}

Figure~\ref{figure:panGraph} shows how this default panning function
affects the left and right channel of a stereo signal.

\begin{centeredFigure}
  \centeredExternalPicture{0.85}{panGraph.mps}
  \caption{Default Panning Function for Left and Right Channels}
  \label{figure:panGraph}
\end{centeredFigure}

After panning and mixing the target file is stored in the
\embeddedCode{audioTargetDirectoryPath} with its name constructed as
the concatenation of \embeddedCode{targetFileNamePrefix},
\embeddedCode{fileNamePrefix} and suffix ``-ALL.m4a''.

But: you do not want a backing track with all voices of your
arrangement, but the ones to be played live should be missing and
ideally you should be able to switch them on and off!

Again we specify this by several mapping variables in the
configuration file.

The first variable, \embeddedCode{audioGroupToVoicesMap}, specifies a
partitioning of the audio voices into groups where some freely
selectable audio group names are mapped onto sets of audio voice
names.

\begin{configurationFileCode}
  audioGroupToVoicesMap = "{"
      " base : bass/keyboard/keyboardSimple/strings,"
      " voc  : vocals/bgVocals,"
      " gtr  : guitar,"
      " drm  : drums/percussion"
   "}"
\end{configurationFileCode}

The voice names in the song should be a subset of the voice names
mentioned in the audio group map; missing or extraneous voice names
will simply be ignored.  When defining those settings globally for a
group of songs, ensure that typical voice name variants (like, for
example, ``keyboardSimple'') are included in one of the lists;
otherwise those voices will be missing in the mix files and videos.

The second variable, \embeddedCode{audioTrackList}, specifies all
tracks that will later occur as tracks in the video, but also that are
rendered as compressed audio files.

Each track is described by a track descriptor with several fields as
shown in figure~\ref{figure:configFileAudioTrackVariables}.  It
consists of a list of the several groups to be combined, templates for
the audio file and the song name, an album name, and some description
and a language code for the video track.  Also there is some audio
information about the specific volume levels for each voice, the
mastering effects for this voice and the final amplification level.

\begin{centeredFigure}
  \begin{variableDescriptionTableShort}

    \varDescriptionShort{audioGroupList}
                        {\DESCaudioTrackGroupList}

    \varDescriptionShort{audioFileTemplate}
                        {\DESCaudioTrackAudioFileTemplate}

    \varDescriptionShort{songNameTemplate}
                        {\DESCaudioTrackSongNameTemplate}

    \varDescriptionShort{albumName}
                        {\DESCaudioTrackAlbumName}

    \varDescriptionShort{description}
                        {\DESCaudioTrackDescription}

    \varDescriptionShort{languageCode}
                        {\DESCaudioTrackLanguageCode}

    \varDescriptionShort{voiceNameTo\-MixSettingMap}
                        {\DESCaudioTrackVoiceNameToMixSettingMap}

    \varDescriptionShort{masteringEffectList}
                        {\DESCaudioTrackMasteringEffectList}

    \varDescriptionShort{amplificationLevel}
                        {\DESCaudioTrackAmplificationLevel}

  \end{variableDescriptionTableShort}

  \caption{Parameters for Audio Track in \embeddedCode{audioTrackList}
           Variable}
  \label{figure:configFileAudioTrackVariables}
\end{centeredFigure}

``Language code'' sounds a bit strange: why do you need that?

Unfortunately not many video players support audio track description
texts for MP4 videos, but most of them allow to select audio tracks by
``language''.  So the audio tracks in the final video are tagged with
both description and language code for some kind of identification.
Of course, the selected languages are quite arbitrary, because you
typically do not find a connection between a list of audio voice names
and some language name.  So you must be creative\dots

\begin{centeredFigure}
  \centeredExternalPicture{0.8}{trackMixdown.mps}
  \caption{Audio Flow during Track Mixdown}
  \label{figure:trackMixdown}
\end{centeredFigure}

The final stage of audio processing is described by several attributes
in the entry for a single audio track within the list of tracks:
\embeddedCode{amplificationLevel},
\embeddedCode{voiceNameToMixSettingMap} and
\embeddedCode{masteringEffectList}.  Figure~\ref{figure:trackMixdown}
illustrates how the audio voice files from the ``refinedaudio'' phase
are combined into the several audio tracks by the mix phase.  In
principle the mix levels per voice can be individual per audio track
as well as its mastering effects and its final level, but of course
there is no adaptation of the voice files done: they are taken
unchanged from the previous phase.

Nevertheless you can also define global settings for all of those and
reference them in the audio track list variable.  Especially the mix
settings map may be global, because the track specific mapping will
only use the levels of those voices defined in its associated audio
groups.

In the configuration file we can define auxiliary variables for the
audio processing:

\begin{configurationFileCode}
  _voiceNameToMixSettingMap = "{"
      " bass : -6, keyboard : -10.5, keyboardSimple : -14,"
      " strings : -2, vocals : 0, bgVocals : -1,"
      " guitar : -4.5, drums : 1.6, percussion : 0"
    "}"
  _masteringEffectList = ""
  _amplificationLevel = -1.2
\end{configurationFileCode}

Note that an individual mix setting may also contain a pan
specification (separated by a slash).  Hence ``bass : -6/0.3R'' would
also be okay and overrides the pan specification given as a list with
variable \embeddedCode{panPositionList}.

For audio tracks we also define an auxiliary variable each to make
thing more comprehensible.

This is the track with all voices:
\begin{configurationFileCode}
   _audioTrackWithAllVoices =
    "all :   { audioGroupList  : base/voc/gtr/drm,"
    " audioFileTemplate        : '$',"
    " songNameTemplate         : '$ [ALL]',"
    " albumName                : 'Best',"
    " description              : 'all voices',"
    " languageCode             : eng,"
    " voiceNameToMixSettingMap : "_voiceNameToMixSettingMap","
    " masteringEffectList      : "_masteringEffectList","
    " amplificationLevel       : "_amplificationLevel" }"
\end{configurationFileCode}

This is the track with all voices except for vocals:
\begin{configurationFileCode}
  _audioTrackNoVocals =
    "novoc : { audioGroupList   : base/gtr/drm,"
    " audioFileTemplate        : '$-novoc',"
    " songNameTemplate         : '$ [-V]',"
    " albumName                : 'Best [no vocals]',"
    " description              : 'no vocals',"
    " languageCode             : deu,"
    " voiceNameToMixSettingMap : "_voiceNameToMixSettingMap","
    " masteringEffectList      : "_masteringEffectList","
    " amplificationLevel       : "_amplificationLevel"}"
\end{configurationFileCode}

Both of them are used in the audio track list definition.

\begin{configurationFileCode}
  audioTrackList = "{"
    _audioTrackWithAllVoices ","
    _audioTrackWithNoVocals ","
    ...
  "}"
\end{configurationFileCode}

So any number of audio tracks is possible.  In the example above we
have two (if you ignore the ellipsis!).  If we assume that the target
file name prefix is ``test-'' and that the song has file name prefix
``wonderful\_song'' and is called ``Wonderful Song'', the files have
the following properties:
\begin{enumerate}

  \item The first track contains all voices, it is stored in
        ``test-wonderful\_song.m4a'' with title
        ``Wonderful~Song~[ALL]'' in album ``Best'' and it has
        the description ``all voices'' and an English language tag.

  \item The second track contains all voices except for vocals and bg
        vocals, it is stored in ``test-wonderful\_song-novoc.m4a''
        with title ``Wonderful~Song~[-V]'' in album
        ``Best~[no~vocals]'' and it has the description ``no vocals''
        and a German language tag.

\end{enumerate}

Figure~\ref{figure:configFileMixVariables} shows the variables
introduced in this section in summary.

\begin{centeredFigure}
  \begin{variableDescriptionTableLong}

    \varDescriptionHeaderLong

    \varDescriptionLong{audioGroupToVoicesMap}
                       {\DESCaudioGroupToVoicesMap}
                       {see text}

    \varDescriptionLong{audioTargetDirectoryPath}
                       {\DESCaudioTargetDirectoryPath}
                       {"/pathto/XXX"}

    \varDescriptionLong{audioTrackList}
                       {\DESCaudioTrackList}
                       {see text}

  \end{variableDescriptionTableLong}

  \caption{Mix Configuration File Variables}
  \label{figure:configFileMixVariables}
\end{centeredFigure}

%.....................................................
\subsubsection{Video Generation: ``finalvideo'' Phase}
%.....................................................

The still videos from the lilypond fragment file contain rendered
score images from lilypond with appropriate display times.  The
``finalvideo'' phase combines those silent videos with the subtitle
file and the rendered audio tracks from above.

There are no big surprises here: for every video file kind in the list
given by \embeddedCode{videoFileKindMap} a video is built with the
following parts:

\begin{itemize}

  \item the file-kind-specific still video (without sound) with the
        appropriate extension \embeddedCode{fileNameSuffix} for the
        given target name finally located in
        \embeddedCode{targetDirectoryPath},

  \item the subtitle file located in
        \embeddedCode{targetDirectoryPath}, and

  \item the compressed audio files generated by the ``mix'' phase
        and located in \embeddedCode{audioTargetDirectoryPath}
\end{itemize}

If \embeddedCode{subtitlesAreHardcoded} is set for the target, the
subtitle is burnt into the video with specified
\embeddedCode{subtitleFontSize} and \embeddedCode{subtitleColor}.
Otherwise the subtitle is put into the target video as a subtitle
track (to be switched on or off).  In the latter case, the rendering
 of the subtitle is done by the video player.

The name of the combined video is constructed from several variables
as follows: the \embeddedCode{targetFileNamePrefix} is concatenated
with \embeddedCode{fileNamePrefix} for the song, a minus character,
the video file kind name suffix and ``.mp4'' extension.  It is stored
in the directory given by \embeddedCode{videoFileKind.directoryPath}.

For example, by those conventions the ``Wonderful Song'' for the
``tablet'' has name ``test-wonderful\_song-tablet.mp4'' and is stored
in the directory given in the target definition.

%----------------
\section{Summary}
%----------------

We're done! We have achieved the following results from a lilypond
fragment file with song voices and a song configuration file:

\begin{itemize}
  \item notation extracts of selected voices as PDF files,
  \item a notation score of selected voices as a PDF file,
  \item a MIDI file with selected voices slightly humanized,
  \item several single voice audio files,
  \item audio file mixes combining voices into groups, and
  \item video files for different target devices containing selectable
        audio tracks and possibly a selectable subtitle with measure
        indication
\end{itemize}

%======================
\chapter{Example}
\label{section:example}
%======================

As the example we take a twelve-bar blues in E with two verses and
some intro and outro.  Note that this song is just an example, its
musical merit is limited.

In the following we shall work with two files:
\begin{itemize}

  \item a song-specific configuration file containing the settings
        for the song (like, for example, the title of the song or the
        voice names) plus some overall settings (like for example, the
        path to programs), and

  \item a lilypond music file containing the music fragments used by
        the generator.

\end{itemize}

Often the single configuration file is split into a song-specific
fragment including overall settings files thus keeping global and
song-specific stuff separate.  For the example we only use a single
configuration file and rely on default settings.

In the following we explain the lilypond fragment file and the
configuration file in pieces; the complete versions are in the
distribution.

%---------------------------------------
\section{Example Lilypond Fragment File}
%---------------------------------------

The lilypond fragment file starts with the inclusion of the note name
language file (using e.g. ``ef'' for \(e\flat\) or ``cs'' for
\(c\sharp\)); additionally the first musical definition is the key and
time designation of the song: it is in e major and uses common time.

\begin{lilypondFileCode}
  \include "english.ly"
  keyAndTime = { \key e \major  \time 4/4 }
\end{lilypondFileCode}

The chords are those of a plain blues with a very simple intro and
outro.  Note that the chords differ for extract and other notation
renderings: for the extract and score we use a volta repeat for the
verses, hence in that case all verse lyrics are stacked vertically and
we only have one pass of the verse.

All chords are generic: there is no distinction by instrument.

\begin{lilypondFileCode}
  chordsIntro = \chordmode { b1*2 | }
  chordsOutro = \chordmode { e1*2 | b2 a2 | e1 }
  chordsVerse = \chordmode { e1*4 | a1*2 e1*2 | b1 a1 e1*2 }
  allChords = {
    \chordsIntro  \repeat unfold 2 { \chordsVerse }
    \chordsOutro
  }
  chordsExtract = { \chordsIntro  \chordsVerse  \chordsOutro }
  chordsScore   = { \chordsExtract }
\end{lilypondFileCode}

\embeddedCode{b1*2} means that it is a B-major chord with a duration
of a whole note (\oneover 1) and this goes for two measures
(``*2''). Analogously there is an \embeddedCode{a2}; this is an
A-major chord with duration of a half note (\oneover 2).  The chords
are repeated twice (\embeddedCode{``\macro{repeat} unfold 2''})
and preceeded by the intro and followed by the outro.

The vocals are simple with a pickup measure.  Because we want to keep
the structure consistent across the voices we have to use two alternate
endings for the \embeddedCode{vocalsExtract} and
\embeddedCode{vocalsScore}.

\begin{lilypondFileCode}
  vocTransition = \relative c' { r4 b'8 as a g e d | }
  vocVersePrefix = \relative c' {
    e2 r | r8 e e d e d b a |
    b2 r | r4 e8 d e g a g | a8 g4. r2 | r4 a8 g a e e d |
    e2 r | r1 | b'4. a2 g8 | a4. g4 d8 d e~ | e2 r |
  }
  vocIntro = { r1 \vocTransition }
  vocVerse = { \vocVersePrefix \vocTransition }
  vocals = { \vocIntro \vocVerse \vocVersePrefix R1*5 }
  vocalsExtract = {
    \vocIntro
    \repeat volta 2 { \vocVersePrefix }
    \alternative {
	{ \vocTransition }{ R1 }
    }
    R1*4
  }
  vocalsScore = { \vocalsExtract }
\end{lilypondFileCode}

The lyrics of the demo song are really bad.  Nevertheless note the
lilypond separation for the syllables and the stanza marks.  For the
video notation the lyrics are serialized.  Because of the pickup
measure, the lyrics have to be juggled around.

\begin{lilypondFileCode}
  vocalsLyricsBPrefix = \lyricmode {
    \set stanza = #"2. " Don't you know I'll go for }
  vocalsLyricsBSuffix = \lyricmode {
    good, be- cause you've ne- ver un- der- stood,
    that I'm bound to leave this quar- ter,
    walk a- long to no- ones home:
    go down to no- where in the end. }
\end{lilypondFileCode}
\begin{lilypondFileCode}
  vocalsLyricsA = \lyricmode {
    \set stanza = #"1. "
    Fee- ling lone- ly now I'm gone,
    it seems so hard I'll stay a- lone,
    but that way I have to go now,
    down the road to no- where town:
    go down to no- where in the end.
    \vocalsLyricsBPrefix }
  vocalsLyricsB = \lyricmode {
    _ _ _ _ _ _ \vocalsLyricsBSuffix }
  vocalsLyrics = { \vocalsLyricsA \vocalsLyricsBSuffix }
  vocalsLyricsVideo = { \vocalsLyrics }
\end{lilypondFileCode}

The bass simply hammers out eighth notes.  As before there is an
extract and a score version with volta repeats and an unfolded version
for the rest (for MIDI and the videos).

\begin{lilypondFileCode}
  bsTonPhrase  = \relative c, { \repeat unfold 7 { e,8  } fs8 }
  bsSubDPhrase = \relative c, { \repeat unfold 7 { a8 } gs8 }
  bsDomPhrase  = \relative c, { \repeat unfold 7 { b8 } cs8 }
  bsDoubleTonPhrase = { \repeat percent 2 { \bsTonPhrase } }
  bsOutroPhrase = \relative c, { b8 b b b a a b a | e1 | }
  bsIntro = { \repeat percent 2 { \bsDomPhrase } }
  bsOutro = { \bsDoubleTonPhrase  \bsOutroPhrase }
  bsVersePrefix = {
    \repeat percent 4 { \bsTonPhrase }
    \bsSubDPhrase \bsSubDPhrase \bsDoubleTonPhrase
    \bsDomPhrase \bsSubDPhrase \bsTonPhrase
  }
  bsVerse = { \bsVersePrefix \bsTonPhrase }

  bass = { \bsIntro  \bsVerse \bsVerse  \bsOutro }
  bassExtract = {
    \bsIntro
    \repeat volta 2 { \bsVersePrefix }
    \alternative {
      {\bsTonPhrase} {\bsTonPhrase}
    }
    \bsOutro
  }
  bassScore = { \bassExtract }
\end{lilypondFileCode}

The guitar plays arpeggios.  As can be seen here, very often the
lilypond macro structure is similar for different voices.

\begin{lilypondFileCode}
  gtrTonPhrase  = \relative c { e,8 b' fs' b, b' fs b, fs }
  gtrSubDPhrase = \relative c { a8 e' b' e, e' b e, b }
  gtrDomPhrase  = \relative c { b8 fs' cs' fs, fs' cs fs, cs }
  gtrDoubleTonPhrase = { \repeat percent 2 { \gtrTonPhrase } }
  gtrOutroPhrase = \relative c { b4 fs' a, e | <e b'>1 | }
  gtrIntro = { \repeat percent 2 { \gtrDomPhrase } }
  gtrOutro = { \gtrDoubleTonPhrase | \gtrOutroPhrase }
  gtrVersePrefix = {
    \repeat percent 4 { \gtrTonPhrase }
    \gtrSubDPhrase  \gtrSubDPhrase  \gtrDoubleTonPhrase
    \gtrDomPhrase  \gtrSubDPhrase  \gtrTonPhrase
  }
  gtrVerse = { \gtrVersePrefix \gtrTonPhrase }
  guitar = { \gtrIntro  \gtrVerse  \gtrVerse  \gtrOutro }
  guitarExtract = {
    \gtrIntro
    \repeat volta 2 { \gtrVersePrefix }
    \alternative {
      {\gtrTonPhrase} {\gtrTonPhrase}
    }
    \gtrOutro
  }
  guitarScore = { \guitarExtract }
\end{lilypondFileCode}

Finally the drums do some monotonic blues accompaniment.  We have to
use the \embeddedCode{myDrums} name here, because \embeddedCode{drums}
is a predefined name in lilypond.  There is no preprocessing of the
lilypond fragment file that could fix this: the fragment is just
included into some boilerplate code, hence it must be conformant to
the lilypond syntax.

\begin{lilypondFileCode}
  drmPhrase = \drummode { <bd hhc>8 hhc <sn hhc> hhc }
  drmOstinato = { \repeat unfold 2 { \drmPhrase } }
  drmFill = \drummode { \drmPhrase tomh16 tomh tommh tommh
                        toml toml tomfl tomfl }
  drmIntro = { \drmOstinato  \drmFill }
  drmOutro = \drummode {
    \repeat percent 6 { \drmPhrase } | <sn cymc>1 | }
  drmVersePrefix = {
    \repeat percent 3 { \drmOstinato }  \drmFill
    \repeat percent 2 { \drmOstinato  \drmFill }
    \repeat percent 3 { \drmOstinato }
  }
  drmVerse = { \drmVersePrefix \drmFill }

  myDrums = { \drmIntro  \drmVerse \drmVerse  \drmOutro }
  myDrumsExtract = { \drmIntro
    \repeat volta 2 {\drmVersePrefix}
    \alternative {
     {\drmFill} {\drmFill}
    }
    \drmOutro }
  myDrumsScore = { \myDrumsExtract }
\end{lilypondFileCode}

So we are done with the lilypond fragment file.  What we have defined
are
\begin{itemize}
  \item the song key and time,
  \item the chords,
  \item the vocal lyrics, and
  \item voices for vocals, bass, guitar and drums.
\end{itemize}

All those definitions take care that the notations shall differ in our
case for extracts/score and other notation renderings.

%-----------------------------------
\section{Example Configuration File}
%-----------------------------------

Our configuration file contains global settings as well as
song-specific settings.

As a convention we prefix auxiliary variable with an underscore to
distinguish them from the real configuration variables.

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\subsection{Overall Configuration - Part 1}
\label{section:exampleOverallConfiguration}
%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If the programs are in special locations one has to define the
specific paths for them.  When they are however reachable by the
system's program path (which is normally the case) nothing has to be
done.  But this is not completely true, because
\embeddedCode{midiToWavRenderingCommandLine} needs special handling:
this is necessary because for fluidsyth as WAV renderer we have to
specify the soundfont location (via a temporary variable).

\begin{configurationFileCode}
  _soundFonts = "/usr/local/midi/soundfonts/FluidR3_GM.SF2"
  midiToWavRenderingCommandLine =
      "fluidsynth -n -i -g 1 -R 0"
      " -F ${outfile} " _soundFonts " ${infile}"
\end{configurationFileCode}

Other global settings would define paths for files or directories, but
for most settings we rely on the defaults.  But we want the temporary
lilypond file to go to ``temp'' (and have some parts in the name for
phase and voice name), the generated PDF and MIDI files to go to
subdirectory ``generated'' of the current directory and audio into
``mediafiles'').  Note that those directories have to be created
manually before running the program, since it checks for their
existence before doing something.

\begin{configurationFileCode}
  tempLilypondFilePath = "./temp/temp_${phase}_${voiceName}.ly"
  intermediateFileDirectoryPath = "./temp"
  targetDirectoryPath = "./generated"
  tempAudioDirectoryPath = "./mediafiles"
\end{configurationFileCode}

Also the default notation settings are fine: they ensure that drums
use the drum staff, that the clefs for bass and guitar have the voices
transposed by an octave up resp.\ down and that drums have no clef at
all.  Chords shall be shown for all extracts of melodic instruments
and on the top voice ``vocals'' in the score and video.  If this were
not okay, we'd have to adapt the variables
\embeddedCode{phaseAndVoiceNameToStaffListMap},
\embeddedCode{phaseAndVoiceNameToClefMap} and
\embeddedCode{voiceNameToChordsMap} from
section~\ref{subsubsection:notation} and
figure~\ref{figure:configFileNotationVariables}.

But the humanization for the MIDI and audio files must be defined for
this song.  It is quite simple: we use a rock groove with tight hits
on two and four and slight timing variations for other positions
within a measure.  Those timing variations are very subtle as the
maximum variation specified is 0.3 \(1/32^{nd}\) notes.

As the velocity variation there is a hard accent on two and a slighter
accent on four while the other positions are much weaker.

We have \emph{not} defined individual variation factors per
instrument; hence all humanized instruments have similar variations in
timing and velocity.

\begin{configurationFileCode}
  countInMeasureCount = 2
  humanizationStyleRockHard  =
    "{ 0.00: 0.95/A0.2, 0.25: 1.15/0,"
    "  0.50: 0.98/0.3, 0.75: 1.1/0,"
    "  OTHER: 0.85/0.25,"
    "  SLACK:0.1, RASTER: 0.03125 }"
\end{configurationFileCode}

The video generation uses the default single video target called
``tablet'' with a landscape orientation of 640x480 and yellow
subtitles, hence there is nothing to be specified in the configuration
file.

For the transformation from midi tracks to audio files there are four
simple sound style definitions: a crunchy bass and guitar, some gritty
drums and distortion for the vocals emulation.  They use overdrive,
some sound shaping and also a bit of compression.  Details of the
parameters can be found in the sox
documentation~\cite{reference:soxDocumentation}.

\begin{configurationFileCode}
  soundStyleBassCrunch =
      " compand 0.05,0.1 6:-20,0,-15"
      " highpass -2 60 1o  lowpass -2 800 1o  equalizer 120 1o +3"
      " reverb 60 100 20 100 10"
  soundStyleDrumsGrit = "overdrive 4 0  reverb 25 50 60 100 40"
  soundStyleGuitarCrunch =
      " compand 0.01,0.1 6:-10,0,-7.5 -6"
      " overdrive 30 0  gain -10"
      " highpass -2 300 0.5o  lowpass -1 1200"
      " reverb 40 50 50 100 30"
  soundStyleVocalsSimple = " overdrive 5 20"
\end{configurationFileCode}

For the final audio files we have two variants: one with all voices,
the other one with missing vocals and background vocals (the ``karaoke
version'').  The song and album names have the appropriate info in
brackets.

All songs and the video will go to the ``mediaFiles'' subdirectory.
Audio and video files have ``test-'' as their prefix before the song
name.  So, for example, the audio file for ``Wonderful Song'' with all
voices has path ``./mediaFiles/test-wonderful\_song.m4a''.

\begin{configurationFileCode}
  targetFileNamePrefix = "test-"
  albumArtFilePath = "./mediaFiles/demo.jpg"

  audioGroupToVoicesMap = "{"
      " base : bass/drums, voc : vocals, gtr : guitar"
  "}"
\end{configurationFileCode}

When all the global settings would be in a specific file, now were the
place where to split this into a preceding file and a file following
the song-specification.

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\subsection{Song-Specific Configuration}
%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

There is not much left to define the song.  First come the overall
properties (where we rely on the defaults as much as possible).

\begin{configurationFileCode}
  title = "Wonderful Song"
  fileNamePrefix = "wonderful_song"
  composerText = "arranged by Fredo, 2021"
  artistName = "Fredo"
  albumName = "Best of Fredo"
\end{configurationFileCode}

The main information about a song is given in the table of voices with
the voice names, midi data, reverb levels and the sound variants.  All
voices have audio postprocessing, nothing is merely copied.  The midi
channels are at their defaults meaning 10 for drums and arbitrary other
values for non-drums.

\begin{configurationFileCode}
  voiceNameList       = "vocals,    bass,  guitar,   drums"
  midiInstrumentList  = "    18,      35,      26,      13"
  midiVolumeList      = "   100,     120,      70,     110"
  panPositionList = "     C,    0.5L,    0.6R,    0.1L"
  reverbLevelList     = "   0.3,     0.0,     0.0,     0.0"
  soundVariantList    = "SIMPLE,  CRUNCH,  CRUNCH,    GRIT"
\end{configurationFileCode}

The audio levels and pan positions are given in a separate mapping,
which is used in the audio track list.  We use a single mapping for
all targets, that means the relative levels and pan positions are
identical in all mixes.

\begin{configurationFileCode}
  _voiceNameToMixSettingMap =
    "{ vocals : -4, bass : 0, guitar : -6, drums : -2 }"
\end{configurationFileCode}

Note that the above definition must come before the
\embeddedCode{audioTrackList} definition.

We also have lyrics: two lines of lyrics in the vocals extract and
score, one (serialized) line in the video.

\begin{configurationFileCode}
  voiceNameToLyricsMap = "{ vocals : e2/s2/v }"
\end{configurationFileCode}

Humanization relies on the humanization style defined in
section~\ref{section:exampleOverallConfiguration}.  It applies to all
voices except vocals and starts in measure 1.

\begin{configurationFileCode}
  humanizedVoiceNameSet = "bass, guitar, drums"
  measureToHumanizationStyleNameMap =
      "{ 1 : humanizationStyleRockHard }"
\end{configurationFileCode}

The overall tempo is 90bpm throughout the song.

\begin{configurationFileCode}
  measureToTempoMap = "{ 1 : 90 }"
\end{configurationFileCode}

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\subsection{Overall Configuration - Part 2}
%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Because we want to set the \embeddedCode{audioTrackList} variable to
non-default (default is one track with all voices), this must come
\emph{after} the song parameters, because it relies on the voice name
to mix settings mapping.

For a separate global file this means, it has to be included as
another fragment \emph{after} the song-specific setting.  Since we are
using a single file, this just comes at the end of the file.

We have two tracks: one with all voices and, one without the vocals;
for convenience we put them each into an auxiliary variable (but this
is not mandatory).

\begin{configurationFileCode}
  _audioTrackWithAllVoices =
    "all : { audioGroupList     : base/voc/gtr,"
    " audioFileTemplate         : '$',"
    " songNameTemplate          : '$ [ALL]',"
    " albumName                 : '$',"
    " description               : 'all voices',"
    " languageCode              : deu,"
    " voiceNameToMixSettingMap : "_voiceNameToMixSettingMap"}"
\end{configurationFileCode}

\begin{configurationFileCode}
  _audioTrackWithoutVocals =
    "novocals : { audioGroupList : base/gtr,"
    " audioFileTemplate          : '$-v',"
    " songNameTemplate           : '$ [-V]',"
    " albumName                  : '$ [-V]',"
    " description                : 'no vocals',"
    " languageCode               : eng,"
    " voiceNameToMixSettingMap   : "_voiceNameToMixSettingMap"}"
\end{configurationFileCode}

Both are combined into the \embeddedCode{audioTrackList}.

\begin{configurationFileCode}
  audioTrackList = "{"
    _audioTrackWithAllVoices ","
    _audioTrackWithoutVocals
  "}"
\end{configurationFileCode}

The separate variable \embeddedCode{\_voiceNameToMixSettingMap}
defined above defines the audio level (and optionally the pan
positions) for all voices; there are no special mastering effects and
all amplification levels are (the default) 0dB.

%--------------------------------
\section{Putting it All Together}
%--------------------------------

Assuming that the configuration is in file
``wonderful\_song-config.txt'' and the lilypond stuff is in
``wonderful\_song-music.ly'', the command to produce everything is

\begin{commandLine}
  lilypondToBVC --phases all wonderful_song-config.txt
\end{commandLine}

and it produces the following target files
\begin{itemize}
  \item in directory ``generated'' the extracts
        ``wonderful\_song-bass.pdf'',
        ``won\-der\-ful\_song-\-drums.pdf'',
        ``wonderful\_song-guitar.pdf'' and
        ``wonder\-ful\_\-song-vocals.pdf'',

  \item the score file ``generated/wonderful\_song\_score.pdf'',

  \item the midi file ``generated/wonderful\_song-std.mid'',

  \item in directory ``~/mediaFiles'' the audio files
        ``test-wonderful\_song.m4a'' and ``test-wonderful\_song-v.m4a'',
        and

  \item the video file with two audio tracks
        ``~/videos/test-wonderful\_song-tblt.mp4''
\end{itemize}

Figure~\ref{figure:exampleTargetImages} shows an extract page (a), one
image of the target video (b) and the first score page (c) as an
illustration.

\begin{centeredFigure}
  \begin{center}
    \begin{minipage}[b]{6.5cm}
      a) \externalPicture{0.14}{demo-extract.png}\\*[3mm]
      b) \externalPicture{0.13}{d.png}
    \end{minipage}
    \hspace*{3mm}
    c) \externalPicture{0.14}{demo-score.png}
  \end{center}

  \caption{Examples for Target File Images}
  \label{figure:exampleTargetImages}
\end{centeredFigure}

%========================
\chapter{Debugging}
\label{section:debugging}
%========================

Several tools are orchestrated by the script and typically something
goes wrong.  The script or one of the underlying tools issues some
error message, but how can you find out what really went wrong?

The first place to look is the logging file located in
\embeddedCode{loggingFilePath} or in the path given by the
\embeddedCode{-l} option on the command-line.  It does a very
fine-grained tracing of the relevant function calls and the last lines
should give you some indication about the error.

Note that the outputs of the called programs are not logged, but at
least the commandlines to call them.  This would not be helpful in
itself, because typically those programs work on generated
intermediate files.  But you can tell ltbvc to keep the intermediate
files by setting \embeddedCode{intermediateFilesAreKept} to true or
alternatively calling the program with the ``-k'' flag.  This only
applies to the preprocessing phases, because in the postprocessing
phases all files are kept as they serve as input for other phases
\footnote{The silent videos and the subtitle file also go into the
          intermediate file directory, because they are not
          interesting in themselves, but must be kept.}.

For example, assume that the score generation phase does not produce a
meaningful output.  If you have set the keep-files-flag, then a file
called ``temp.ly'' is produced and kept that contains the boiler-plate
code for the score.  You can then run
\begin{commandLine}
  lilypond test.ly
\end{commandLine}
and see what happens.  Of course, you must be able to get by with the
lilypond messages, but this is plain lilypond expertise.

Assuming default settings of the configuration variables, the
following temporary files will be produced:

\begin{ttDescription}
  \item [extract:] a single temp.ly file containing a single voice,

  \item [score:] a single temp.ly file for the complete score,

  \item [midi:] a single temp.ly file for the midi voices and a
        generated ``.mid'' file containing the voices with standard
        sound assignment and no humanization, and

  \item [silentvideo:] a single temp.ly file for the video voices,
        ``.png'' image files with single pages of the video and
        ``.mp4'' files containing the parts of the video showing just
        a single page.
\end{ttDescription}

For the postprocessing phases all intermediate files are kept as follows:
\begin{ttDescription}

  \item [rawaudio:] each voice wave-file goes into the path specified
        by \embeddedCode{tempAudioDirectoryPath} as ``\meta{voice}.wav'',

  \item [refinedaudio:] each voice wave-file goes into the path
        specified by \embeddedCode{tempAudioDirectoryPath} as
        ``\meta{voice}-processed.wav'',

  \item [mix and finalvideo:] both phases only have target files
        in \embeddedCode{audioTargetDirectoryPath} and the target
        specific path in \embeddedCode{targetVideoDirectory}.

\end{ttDescription}

Most problems in postprocessing probably occur in the ``refinedaudio''
phase, because sox does a lot of complex transformations.  It might be
helpful to insert ``tee'' commands in the sox processing chain in the
command file to have a peek at intermediate audio stages.

Be aware that ``tee'' is not a standard sox command: if you execute
the sox steps directly on the command line, you must take care of any
intermediate files yourself.

%===============================
\chapter{Future Extensions}
\label{section:futureExtensions}
%===============================

The following things are not contained in the current version, but are
planned for future versions:

\begin{itemize}

  \item The sound variant list (describing a single sound variant for
        each voice) shall be replaced by map from voice to a map from
        measure to sound variant.  This allows to have individual
        sound styles for different parts in a song (like, for example,
        for an instrument solo part where special sounds are required).

  \item The algorithm for finding the measures for the page breaks for
        the video is quite naive and fragile.  The page breaks are
        currently found by scanning the Lilypond Postscript file,
        because to my knowledge Lilypond currently has no means for
        providing the location of those breaks programmatically.  Some
        better solution must be found.

  \item Currently the humanization algorithm can only cope with a
        single time signature for the complete song and uses the same
        (measure-specific) humanization pattern for all voices.
        Similarly to the sound variants a map should be used from
        measure and voice to the humanization pattern.

  \item In professional audio productions drums are processed by
        handling the different drum instrument groups (e.g. kick,
        snare, toms, cymbals) individually.  This is currently not
        possible: drums are simply a single audio voice.  A workaround
        could be done, if the used midi-to-wav-converter produced a
        multi-channel result and the refinement stage were able to
        combine those parts into a final result.  But possibly this
        should go into the workflow itself.

  \item It is not clear whether the modelled workflow is really
        adequate for a band setting.  Many steps (e.g. the different
        submixes) are similar to aux busses in an analog mixer, but in
        digital mixers also pan position or even equalization of
        voices may be adapted for the submix.  This cannot be achieved
        now.

  \item If you use this setup feeding a mixer from e.g. a tablet,
        there is only one device available.  If other band members
        have different videos, there is currently no way to
        synchronize them (e.g. via a time code).

\end{itemize}

%=========================
\chapter{References}
\label{section:references}
%=========================

\begingroup
\renewcommand{\chapter}[2]{}%
\bibliographystyle{plain}
\begin{thebibliography}{SFNT-ORIG}

    \bibitem[AAC]{reference:aacDocumentation}
        \textit{QAAC - Quicktime AAC}.\\
        \hyperlink{https://sites.google.com/site/qaacpage/}

    \bibitem[FFMPEG]{reference:ffmpegDocumentation}
        \textit{FFMPEG - Documentation}.\\
        \hyperlink{http://ffmpeg.org/documentation.html}

    \bibitem[FLUID]{reference:fluidsynthDocumentation}
        \textit{FluidSynth - Software synthesizer based on the
                SoundFont~2 specifications}.\\
        \hyperlink{http://fluidsynth.org}

    \bibitem[LILY]{reference:lilypondDocumentation}
        \textit{Lilypond - Music Notation for Everyone}.\\
        \hyperlink{http://lilypond.org}

    \bibitem[MP4BOX]{reference:mp4boxDocumentation}
        \textit{GPAC - General Documentation MP4Box}.\\
        \hyperlink{https://gpac.wp.imt.fr/mp4box/mp4box-documentation/}

    \bibitem[SFNT-ORIG]{reference:fluidSoundfontOriginal}
        \textit{FluidR3\_GM.sf2 SoundFont at archive.org}.\\
        \hyperlink{https://archive.org/compress/fluidr3-gm-gs}

    \bibitem[SFNT-MS]{reference:fluidSoundfontMuseScore}
        \textit{FluidR3\_GM.sf3 SoundFont at musescore.org}.\\
        \hyperlink{https://github.com/musescore/MuseScore/raw/2.1/share/sound/FluidR3Mono\_GM.sf3}

    \bibitem[SOX]{reference:soxDocumentation}
        Chris Bagwell, Lance Norskog et al.:
        \textit{SoX - Sound eXchange - Documentation}.\\
        \hyperlink{http://sox.sourceforge.net/Docs/Documentation}

\end{thebibliography}
\endgroup

\appendix

%=================================================
\chapter{Table of Configuration File Variables}
\label{section:configurationFileVariableList}
%=================================================

The following table describes all the configuration variables with
their default values and the figure numbers where those variables have
been mentioned first in the current document.

\newcommand{\bdot}{\linebreak[0].}

\begin{variableDescriptionTableFinal}

    \varDescriptionHeaderFinal

    \varDescriptionFinal{aacCommandLine}
                        {\DESCaacCommandLine}
                        {empty, will be replaced by ffmpeg}
                        {Program}

    \varDescriptionFinal{albumName}
                        {\DESCalbumName}
                        {"UNKNOWN ALBUM"}
                        {SongGroupRelated}

    \varDescriptionFinal{artistName}
                        {\DESCartistName}
                        {"UNKNOWN ARTIST"}
                        {SongGroupRelated}

    \varDescriptionFinal{audioGroupToVoicesMap}
                        {\DESCaudioGroupToVoicesMap}
                        {single group "all" mapped to set of all voice
                         names in \embeddedCode{voiceNameList} plus
                         a group for each voice with the same name}
                        {Mix}

    \varDescriptionFinal{audioProcessor\bdot amplificationEffect}
                        {\DESCaudioProcessorAmplificationEffect}
                        {"gain \placeholder{amplificationLevel}"}
                        {AudioProcessor}

    \varDescriptionFinal{audioProcessor\bdot chainSeparator}
                        {\DESCaudioProcessorChainSeparator}
                        {";"}
                        {AudioProcessor}

    \varDescriptionFinal{audioProcessor\bdot mixingCommandLine}
                        {\DESCaudioProcessorMixingCommandLine}
                        {"sox -m [-v \placeholder{factor}
                                     \placeholder{infile}]
                                 \placeholder{outfile}"}
                        {AudioProcessor}

    \varDescriptionFinal{audioProcessor\bdot paddingCommandLine}
                        {\DESCaudioProcessorPaddingCommandLine}
                        {"sox \placeholder{infile}
                              \placeholder{outfile}
                              pad \placeholder{duration}"}
                        {AudioProcessor}

    \varDescriptionFinal{audioProcessor\bdot redirector}
                        {\DESCaudioProcessorRedirector}
                        {"->"}
                        {AudioProcessor}

    \varDescriptionFinal{audioProcessor\bdot refinementCommandLine}
                        {\DESCaudioProcessorRefinementCommandLine}
                        {"sox \placeholder{infile}
                              \placeholder{outfile}
                              \placeholder{effects}"}
                        {AudioProcessor}

    \varDescriptionFinal{audioTargetDirectory\-Path}
                        {\DESCaudioTargetDirectoryPath}
                        {"./mediafiles"}
                        {Mix}

    \varDescriptionFinal{audioTrack\bdot albumName}
                        {\DESCaudioTrackAlbumName}
                        {albumName}
                        {AudioTrack}

    \varDescriptionFinal{audioTrack\bdot amplificationLevel}
                        {\DESCaudioTrackAmplificationLevel}
                        {0}
                        {Mix}

    \varDescriptionFinal{audioTrack\bdot audioFileTemplate}
                        {\DESCaudioTrackAudioFileTemplate}
                        {"\$"}
                        {AudioTrack}

    \varDescriptionFinal{audioTrack\bdot audioGroupList}
                        {\DESCaudioTrackGroupList}
                        {all voice names in \embeddedCode{voiceNameList}}
                        {AudioTrack}

    \varDescriptionFinal{audioTrack\bdot description}
                        {\DESCaudioTrackDescription}
                        {empty}
                        {AudioTrack}

    \varDescriptionFinal{audioTrack\bdot languageCode}
                        {\DESCaudioTrackLanguageCode}
                        {eng}
                        {AudioTrack}

    \varDescriptionFinal{audioTrack\bdot masteringEffectList}
                        {\DESCaudioTrackMasteringEffectList}
                        {empty}
                        {Mix}

    \varDescriptionFinal{audioTrack\bdot songNameTemplate}
                        {\DESCaudioTrackSongNameTemplate}
                        {title}
                        {AudioTrack}

    \varDescriptionFinal{audioTrackList}
                        {\DESCaudioTrackList}
                        {a single audio track with all voices from
                         \embeddedCode{voiceNameList}}
                        {Mix}

    \varDescriptionFinal{audioVoiceNameSet}
                        {\DESCaudioVoiceNameSet}
                        {\embeddedCode{voiceNameList}}
                        {Audio}

    \varDescriptionFinal{composerText}
                        {\DESCcomposerText}
                        {empty}
                        {SongRelated}

    \varDescriptionFinal{countInMeasureCount}
                        {\DESCcountInMeasureCount}
                        {0}
                        {Humanization}

    \varDescriptionFinal{extractVoiceNameSet}
                        {\DESCextractVoiceNameSet}
                        {\embeddedCode{voiceNameList}}
                        {Extract}

    \varDescriptionFinal{ffmpegCommand}
                        {\DESCffmpegCommand}
                        {ffmpeg on system's path otherwise \mandatory}
                        {Program}

    \varDescriptionFinal{fileNamePrefix}
                        {\DESCfileNamePrefix}
                        {\mandatory}
                        {SongRelated}

    \varDescriptionFinal{humanizationStyleXXX}
                        {\DESChumanizationStyleXXX}
                        {empty}
                        {Humanization}

    \varDescriptionFinal{humanizedVoiceNameSet}
                        {\DESChumanizedVoiceNameSet}
                        {empty}
                        {Humanization}

    \varDescriptionFinal{includeFilePath}
                        {\DESCincludeFilePath}
                        {\embeddedCode{fileNamePrefix} plus ``-music.ly''}
                        {Path}

    \varDescriptionFinal{intermediateFilesAreKept}
                        {\DESCintermediateFilesAreKept}
                        {false}
                        {SongRelated}

    \varDescriptionFinal{intermediateFileDirectoryPath}
                        {\DESCintermediateFileDirectoryPath}
                        {current directory}
                        {Path}

    \varDescriptionFinal{lilypondCommand}
                        {\DESClilypondCommand}
                        {lilypond on system's path otherwise \mandatory}
                        {Program}

    \varDescriptionFinal{lilypondVersion}
                        {\DESClilypondVersion}
                        {"2.18.22"}
                        {Program}

    \varDescriptionFinal{loggingFilePath}
                        {\DESCloggingFilePath}
                        {ltbvc.log in current directory}
                        {Path}

    \varDescriptionFinal{measureToHumanization\-StyleNameMap}
                        {\DESCmeasureToHumanizationStyleNameMap}
                        {empty}
                        {Humanization}

    \varDescriptionFinal{measureToTempoMap}
                        {\DESCmeasureToTempoMap}
                        {120 bpm starting at first measure}
                        {SongRelated}

    \varDescriptionFinal{midiChannelList}
                        {\DESCmidiChannelList}
                        {channel 10 for drums and percussion,
                         arbitrary other number for other voices}
                        {MidiRelated}

    \varDescriptionFinal{midiInstrumentList}
                        {\DESCmidiInstrumentList}
                        {some default assignments from General MIDI}
                        {MidiRelated}

    \varDescriptionFinal{midiToWavRendering\-CommandLine}
                        {\DESCmidiToWavRenderingCommandLine}
                        {\mandatory}
                        {Program}

    \varDescriptionFinal{midiVoiceNameList}
                        {\DESCmidiVoiceNameList}
                        {voiceNameList}
                        {MidiRelated}

    \varDescriptionFinal{midiVolumeList}
                        {\DESCmidiVolumeList}
                        {80 for each voice in \embeddedCode{voiceNameList}}
                        {MidiRelated}

    \varDescriptionFinal{mp4boxCommand}
                        {\DESCmpfourboxCommand}
                        {empty, will be replaced by ffmpeg}
                        {Program}

    \varDescriptionFinal{panPositionList}
                        {\DESCpanPositionList}
                        {all voices have center pan position}
                        {MidiRelated}

    \varDescriptionFinal{parallelTrack}
                        {\DESCparallelTrack}
                        {empty}
                        {Audio}

    \varDescriptionFinal{phaseAndVoiceName\-ToClefMap}
                        {\DESCphaseAndVoiceNameToClefMap}
                        {empty}
                        {Notation}

    \varDescriptionFinal{phaseAndVoiceName\-ToStaffListMap}
                        {\DESCphaseAndVoiceNameToStaffListMap}
                        {empty}
                        {Notation}

    \varDescriptionFinal{reverbLevelList}
                        {\DESCreverbLevelList}
                        {0.0 (no reverb) for each voice in
                         \embeddedCode{voiceNameList}}
                        {Audio}

    \varDescriptionFinal{scoreVoiceNameList}
                        {\DESCscoreVoiceNameList}
                        {voiceNameList}
                        {Score}

    \varDescriptionFinal{soundStyleXXX}
                        {\DESCsoundStyleXXX}
                        {empty}
                        {Audio}

    \varDescriptionFinal{soundVariantList}
                        {\DESCsoundVariantList}
                        {"COPY" for each of the voices}
                        {Audio}

    \varDescriptionFinal{targetDirectoryPath}
                        {\DESCtargetDirectoryPath}
                        {current directory}
                        {Path}

    \varDescriptionFinal{tempAudioDirectoryPath}
                        {\DESCtempAudioDirectoryPath}
                        {current directory}
                        {Path}

    \varDescriptionFinal{tempLilypondFilePath}
                        {\DESCtempLilypondFilePath}
                        {"temp\_\placeholder{phase}
                         \_\placeholder{voiceName}.ly"
                         in current directory}
                        {Path}

    \varDescriptionFinal{title}
                        {\DESCtitle}
                        {\mandatory}
                        {SongRelated}

    \varDescriptionFinal{trackNumber}
                        {\DESCtrackNumber}
                        {0}
                        {SongRelated}

    \varDescriptionFinal{videoFileKind\bdot directoryPath}
                        {\DESCvideoFileKindDirectoryPath}
                        {current directory}
                        {VideoFileKind}

    \varDescriptionFinal{videoFileKind\bdot fileNameSuffix}
                        {\DESCvideoFileKindFileNameSuffix}
                        {\mandatory}
                        {VideoFileKind}

    \varDescriptionFinal{videoFileKind\bdot target}
                        {\DESCvideoFileKindTarget}
                        {\mandatory}
                        {VideoFileKind}

    \varDescriptionFinal{videoFileKind\bdot voiceNameList}
                        {\DESCvideoFileKindVoiceNameList}
                        {voiceNameList}
                        {VideoFileKind}


    \varDescriptionFinal{videoTarget\bdot ffmpegPresetName}
                        {\DESCvideoTargetFFMpegPresetName}
                        {``''}
                        {VideoTarget}

    \varDescriptionFinal{videoTarget\bdot frameRate}
                        {\DESCvideoTargetFrameRate}
                        {10}
                        {VideoTarget}

    \varDescriptionFinal{videoTarget\bdot height}
                        {\DESCvideoTargetHeight}
                        {\mandatory}
                        {VideoTarget}

    \varDescriptionFinal{videoTarget\bdot leftRightMargin}
                        {\DESCvideoTargetLeftRightMargin}
                        {\mandatory}
                        {VideoTarget}

    \varDescriptionFinal{videoTarget\bdot mediaType}
                        {\DESCvideoTargetMediaType}
                        {"TV Show"}
                        {VideoTarget}

    \varDescriptionFinal{videoTarget\bdot resolution}
                        {\DESCvideoTargetResolution}
                        {\mandatory}
                        {VideoTarget}

    \varDescriptionFinal{videoTarget\bdot scalingFactor}
                        {\DESCvideoTargetScalingFactor}
                        {1}
                        {VideoTarget}

    \varDescriptionFinal{videoTarget\bdot subtitleColor}
                        {\DESCvideoTargetSubtitleColor}
                        {(yellow)}
                        {VideoTarget}

    \varDescriptionFinal{videoTarget\bdot subtitleFontSize}
                        {\DESCvideoTargetSubtitleFontSize}
                        {10}
                        {VideoTarget}

    \varDescriptionFinal{videoTarget\bdot subtitlesAreHardcoded}
                        {\DESCvideoTargetSubtitlesAreHardcoded}
                        {false}
                        {VideoTarget}

    \varDescriptionFinal{videoTarget\bdot systemSize}
                        {\DESCvideoTargetSystemSize}
                        {20 (default of lilypond)}
                        {VideoTarget}

    \varDescriptionFinal{videoTarget\bdot topBottomMargin}
                        {\DESCvideoTargetTopBottomMargin}
                        {\mandatory}
                        {VideoTarget}

    \varDescriptionFinal{videoTarget\bdot width}
                        {\DESCvideoTargetWidth}
                        {\mandatory}
                        {VideoTarget}

    \varDescriptionFinal{videoTargetMap}
                        {\DESCvideoTargetMap}
                        {\mandatory}
                        {Video}

    \varDescriptionFinal{voiceNameToChordsMap}
                        {\DESCvoiceNameToChordsMap}
                        {empty}
                        {Notation}

    \varDescriptionFinal{voiceNameToLyricsMap}
                        {\DESCvoiceNameToLyricsMap}
                        {empty}
                        {Notation}

    \varDescriptionFinal{voiceNameToOverride\-FileNameMap}
                        {\DESCvoiceNameToOverrideFileNameMap}
                        {empty}
                        {Audio}

    \varDescriptionFinal{voiceNameToScore\-NameMap}
                        {\DESCvoiceNameToScoreNameMap}
                        {empty}
                        {Score}

    \varDescriptionFinal{voiceNameToVariation\-FactorMap}
                        {\DESCvoiceNameToVariationFactorMap}
                        {empty}
                        {Humanization}

    \varDescriptionFinal{year}
                        {\DESCyear}
                        {current year}
                        {SongRelated}

\end{variableDescriptionTableFinal}

%=======================
\chapter{Glossary}
\label{section:glossary}
%=======================

\begin{ttGlossary}
    \item [album]
        \glossaryLink{song group}

    \item [all (phase group)]
        a group of \glossaryLink{processing phase}s doing full
        processing via phase groups \glossaryLink{preprocess} and
        \glossaryLink{postprocess}

    \item [audio group]
        a group of \glossaryLink{voice} \glossaryLink{audio track}s to
        be mixed into a target audio file or into a single audio track
        in the target video files

    \item [audio track]
        the audio rendering of a subset of all song voices (typically
        within the final notation video)

    \item [(song) configuration file]
        a text file containing configuration information for a single
        \glossaryLink{song} (possibly including other text
        configuration files) that is used in generation of wrapper
        \glossaryLink{lilypond} files and parametrization of
        underlying generation programs; consists of key-value pairs
        with variable names as keys followed by an equal sign and a
        string, boolean or numeric value

    \item [(audio) effect]
        a filter applied to audio files during the phases
        \glossaryLink{refinedaudio} and \glossaryLink{mix} to
        transform input audio; typically the program
        \glossaryLink{sox} will provide the necessary filters
        
    \item [extract (phase)]
        a \glossaryLink{processing phase} producing the extract PDF
        notation files for single \glossaryLink{voice}s using the
        program \glossaryLink{lilypond}

    \item [ffmpeg]
        a command-line program for producing videos from notation page
        images, inserting hard subtitles into them and possibly
        combining those silent videos with audio tracks (when
        \glossaryLink{mp4box} is not used for that)

    \item [finalvideo (phase)]
        a \glossaryLink{processing phase} generating final video files
        for each \glossaryLink{video file kind} with all submixes as
        selectable audio tracks and with a measure indication as
        subtitle using the programs \glossaryLink{ffmpeg} and
        optionally \glossaryLink{mp4box}

    \item [fluidsynth]
        a command-line program for conversion of MIDI files into WAV
        audio files (representing \glossaryLink{audio track}s) using
        \glossaryLink{sound font}s

    \item [humanization]
        a part of the \glossaryLink{midi} phase applying algorithmic
        and rule-based random time and volume (velocity) shifts to
        notes in the midi stream of \glossaryLink{voices}

    \item [humanization style]
        the configuration information for \glossaryLink{humanization}
        of a \glossaryLink{song} telling individual variations based
        on the position of a note within a measure; gives timing and
        velocity variations for the main beats, the other sixteenths
        and all other notes; multiple styles may be given for a song
        for non-overlapping measure ranges

    \item [lilypond]
        a typesetting program transforming text files with music
        notation information into PDF or MIDI files

    \item [lilypond fragment file]
        a text file with fragmentary \glossaryLink{lilypond} typesetting
        information; based on a song-specific
        \glossaryLink{configuration file} the generator provides wrapping
        lilypond code and calls the appropriated underlying programs

    \item [midi (phase)]
        a \glossaryLink{processing phase} producing a MIDI file
        containing all \glossaryLink{voice}s with specified
        instruments, pan positions and volumes using the program
        \glossaryLink{lilypond} plus some \glossaryLink{humanization}

    \item [mix (phase)]
        a \glossaryLink{processing phase} generating final compressed
        audio files with submixes of all instrument
        \glossaryLink{voice}s based on the refined audio files with
        specified volume balance and some subsequent mastering audio
        processing (where the submix variants are configurable)
        typically using the program \glossaryLink{sox}

    \item [mp4box]
        a command-line program for combining the silent notation
        videos with \glossaryLink{audio track}s; used optionally instead
        of \glossaryLink{ffmpeg} for a better compatibility with Apple
        devices

    \item [override (of a voice audio)]
        a replacement of the refined audio file for some
        \glossaryLink{voice} by an external audio file to be applied
        in the \glossaryLink{refinedaudio} phase; is normally applied
        when the external file has a higher quality (like, for
        example, with a real singer instead of a vocals instrumental
        rendition)

    \item [parallel track (audio)]
        an additional audio file to be added in the
        \glossaryLink{mix} phase; this is used for a single external
        audio file not associated with some voice (like, for example,
        background sounds)

    \item [preprocess (phase group)]
        a group of \glossaryLink{processing phase}s combining
        \glossaryLink{extract}, \glossaryLink{score},
        \glossaryLink{midi} and \glossaryLink{silentvideo} for
        generation of \glossaryLink{voice} extract PDFs and score PDF,
        MIDI file as well the silent videos for all
        \glossaryLink{video file kind}s

    \item [postprocess (phase group)]
        a group of \glossaryLink{processing phase}s combining
        \glossaryLink{rawaudio},
        \glossaryLink{refinedaudio}, \glossaryLink{mix} and
        \glossaryLink{finalvideo} for generation of the intermediate
        raw and refined WAV files, the submixes as compressed audios
        and the final videos for all \glossaryLink{video file kind}s

    \item [processing phase]
        a part of the generation of \glossaryLink{song} artifacts from
        given \glossaryLink{lilypond fragment file} and
        \glossaryLink{configuration file}; possible processing phases
        or processing phase groups are \glossaryLink{all},
        \glossaryLink{preprocess}, \glossaryLink{postprocess},
        \glossaryLink{extract}, \glossaryLink{score},
        \glossaryLink{midi}, \glossaryLink{silentvideo},
        \glossaryLink{rawaudio}, \glossaryLink{refinedaudio},
        \glossaryLink{mix} and \glossaryLink{finalvideo}

    \item [qaac]
        a command-line program for converting WAV audio files into aac
        encoded audio files (representing \glossaryLink{audio groups});
        used optionally instead of \glossaryLink{ffmpeg} for a better
        encoding quality

    \item [rawaudio (phase)]
        a \glossaryLink{processing phase} producing unprocessed
        (intermediate) audio files for all the instrument
        \glossaryLink{voice}s from the midi tracks using the program
        \glossaryLink{fluidsynth} plus some \glossaryLink{sound fonts}

    \item [refinedaudio (phase)]
        a \glossaryLink{processing phase} producing (intermediate)
        audio files for all the instrument \glossaryLink{voice}s
        with additional audio processing applied by the program
        \glossaryLink{sox}

    \item [score (phase)]
        a \glossaryLink{processing phase} producing a single PDF
        notation file containing all \glossaryLink{voice}s as a score
        generated by the program \glossaryLink{lilypond}

    \item [silentvideo (phase)]
        a \glossaryLink{processing phase} to generate (intermediate)
        silent videos containing the score pages for several output
        \glossaryLink{video target}s (with configurable resolution and
        size) using \glossaryLink{ffmpeg} as the video generator from
        notation pages produced by \glossaryLink{lilypond}

    \item [song]
        a collection of several parallel \glossaryLink{voice}s forming
        a musical piece

    \item [song group]
        a collection of several related \glossaryLink{song}s (for
        example, related by year, artist, etc.) sharing common
        characteristics

    \item [sound font (file)]
        a file containing data for a sample-based rendering of MIDI
        data as audio files; the generator uses the
        \glossaryLink{fluidsynth} program for this conversion within
        the \glossaryLink{rawaudio} phase

    \item [sound style]
        a (sequential) chain of \glossaryLink{sox} audio filters to be
        applied to a an audio rendering of a \glossaryLink{voice} in
        phase \glossaryLink{refinedaudio}; typically those sound
        styles are instrument specific

    \item [sox]
        a program for transformation of audio files via parametrizable
        audio \glossaryLink{effect}s (like, for example, equalizers,
        distortions or reverbs) used in the
        \glossaryLink{refinedaudio} and \glossaryLink{mix} phases

    \item [video file kind]
        the configuration information used in the
        \glossaryLink{silentvideo} and \glossaryLink{finalvideo} phases
        giving video rendering properties of notation videos extending
        characteristics of a \glossaryLink{video target} by data
        (like, for example, the list of voices to be shown or the video
        files target directory)

    \item [video target]
        the configuration information used in the
        \glossaryLink{silentvideo} and \glossaryLink{finalvideo}
        phases giving video device dependent properties of notation
        videos (like, for example, device resolution or pixel width
        and height), but also some device independent parameters
        (like, for example, the subtitle font size)

    \item [voice]
        a polyphonic part of a composition belonging to a single
        instrument to be notated in one or several musical staffs

\end{ttGlossary}

%=========================
\chapter{Release Changes}
\label{section:releaseChanges}
%=========================

\begin{itemize}
  \item Version 1.1 (2021-11):
    \begin{itemize}
      \item added static typing tags for additional documentation

      \item professionalized the processing and handling of
        configuration file data by a generic data type management

      \item tried to reduce the mandatory configuration variables as
        much as possible by providing reasonable default settings

      \item added a new logging file command line parameter (overriding
        the setting in the configuration file)

      \item set logging time resolution to 10ms (instead of 1s)

      \item renamed \embeddedCode{keepIntermediateFiles} to
        \embeddedCode{intermediateFilesAreKept}

      \item added several minor corrections in the processing variables
        (e.g. \embeddedCode{tempLilypondFilePath} now has placeholders
        for phase and voice name)

      \item ensured that the temporary MIDI file now uses the
        instruments from the configuration file

      \item made all temporary files go into directory given by
        configuration variable \embeddedCode{intermediateFileDirectoryPath}
        and allowed for a distinctive naming for different phases or
        voices

      \item corrected erroneous AAC processing by ffmpeg

      %\item the example in the documentation now has the configuration
      %  file fragments ordered by the processing phases

    \end{itemize}


  \item Version 1.0 (2018-04): initial version
\end{itemize}

\end{document}
